{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\anaconda3\\lib\\site-packages (4.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from beautifulsoup4) (1.8)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (2.21.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests) (2.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data on IMDb by BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The Shawshank Redemption (1994)', 'The Godfather (1972)', 'The Godfather: Part II (1974)', 'The Dark Knight (2008)', '12 Angry Men (1957)', \"Schindler's List (1993)\", 'The Lord of the Rings: the Return of the King (2003)', 'Pulp Fiction (1994)', 'The Good, the Bad and the Ugly (1966)', 'The Lord of the Rings: the Fellowship of the Ring (2001)', 'Fight Club (1999)', 'Forrest Gump (1994)', 'Inception (2010)', 'Star Wars: Episode V - the Empire Strikes Back (1980)', 'The Lord of the Rings: the Two Towers (2002)', 'The Matrix (1999)', 'Goodfellas (1990)', \"One Flew Over the Cuckoo's Nest (1975)\", 'Seven Samurai (1954)', 'Se7en (1995)', 'City of God (2002)', 'La Vita è Bella (1997)', 'The Silence of the Lambs (1991)', \"It's a Wonderful Life (1946)\", 'Star Wars: Episode IV - a New Hope (1977)', 'Parasite (2019)', 'Saving Private Ryan (1998)', 'Spirited Away (2001)', 'The Green Mile (1999)', 'Interstellar (2014)', 'Léon (1994)', 'The Usual Suspects (1995)', 'Seppuku (1962)', 'The Lion King (1994)', 'American History X (1998)', 'The Pianist (2002)', 'Terminator 2: Judgment Day (1991)', 'Back to the Future (1985)', 'Modern Times (1936)', 'Psycho (1960)', 'Gladiator (2000)', 'City Lights (1931)', 'Intouchables (2011)', 'The Departed (2006)', 'Whiplash (2014)', 'The Prestige (2006)', 'Once Upon a Time in the West (1968)', 'Grave of the Fireflies (1988)', 'Casablanca (1942)', 'Joker (2019)', 'Rear Window (1954)', 'Cinema Paradiso (1988)', 'Alien (1979)', 'Apocalypse Now (1979)', 'Raiders of the Lost Ark (1981)', 'Memento (2000)', 'The Great Dictator (1940)', 'Das Leben Der Anderen (2006)', 'Django Unchained (2012)', 'Paths of Glory (1957)', 'The Shining (1980)', 'Avengers: Infinity War (2018)', 'Wall-E (2008)', 'Sunset Blvd. (1950)', 'Spider-Man: Into the Spider-Verse (2018)', 'Princess Mononoke (1997)', 'Dr. Strangelove or: How I Learned to Stop Worrying and Lo... (1964)', 'Oldboy (2003)', 'Witness for the Prosecution (1957)', 'Avengers: Endgame (2019)', 'The Dark Knight Rises (2012)', 'Once Upon a Time in America (1984)', 'Aliens (1986)', 'Your Name. (2016)', 'Coco (2017)', 'American Beauty (1999)', '1917 (2019)', 'Braveheart (1995)', 'Das Boot (1981)', '3 Idiots (2009)', 'Toy Story (1995)', 'High and Low (1963)', 'Like Stars on Earth (2007)', 'Star Wars: Episode VI - Return of the Jedi (1983)', 'Amadeus (1984)', 'Reservoir Dogs (1992)', 'Inglourious Basterds (2009)', 'Good Will Hunting (1997)', '2001: A Space Odyssey (1968)', 'Requiem for a Dream (2000)', 'Vertigo (1958)', 'M (1931)', 'Dangal (2016)', 'Eternal Sunshine of the Spotless Mind (2004)', 'Citizen Kane (1941)', 'Jagten (2012)', 'Full Metal Jacket (1987)', 'Capernaum (2018)', 'North By Northwest (1959)', 'A Clockwork Orange (1971)', 'Snatch (2000)', 'The Kid (1921)', 'Ladri Di Biciclette (1948)', \"Singin' in the Rain (1952)\", 'Scarface (1983)', 'Taxi Driver (1976)', \"Le Fabuleux Destin D'amélie Poulain (2001)\", 'Lawrence of Arabia (1962)', 'The Sting (1973)', 'Toy Story 3 (2010)', 'Metropolis (1927)', 'Ikiru (1952)', 'For a Few Dollars More (1965)', 'A Separation (2011)', 'Double Indemnity (1944)', 'The Apartment (1960)', 'To Kill a Mockingbird (1962)', 'Incendies (2010)', 'Indiana Jones and the Last Crusade (1989)', 'Up (2009)', 'L.A. Confidential (1997)', 'Monty Python and the Holy Grail (1975)', 'Heat (1995)', 'Rashômon (1950)', 'Die Hard (1988)', 'Yojimbo (1961)', 'Batman Begins (2005)', 'Green Book (2018)', 'Der Untergang (2004)', 'Unforgiven (1992)', 'Children of Heaven (1997)', 'Come and See (1985)', 'Some Like It Hot (1959)', \"Howl's Moving Castle (2004)\", 'Ran (1985)', 'The Great Escape (1963)', 'All About Eve (1950)', 'A Beautiful Mind (2001)', 'Casino (1995)', \"Pan's Labyrinth (2007)\", 'My Neighbour Totoro (1988)', 'El Secreto de Sus Ojos (2009)', 'Raging Bull (1980)', 'Lock, Stock and Two Smoking Barrels (1998)', 'The Wolf of Wall Street (2013)', 'The Treasure of the Sierra Madre (1948)', 'Judgment at Nuremberg (1961)', 'There Will Be Blood (2007)', 'My Father and My Son (2005)', 'Three Billboards Outside Ebbing, Missouri (2017)', 'The Gold Rush (1925)', 'Chinatown (1974)', 'Dial M for Murder (1954)', 'V for Vendetta (2005)', 'The Seventh Seal (1957)', 'Inside Out (2015)', 'No Country for Old Men (2007)', 'Warrior (2011)', 'Shutter Island (2009)', 'Trainspotting (1996)', 'The Elephant Man (1980)', 'The Sixth Sense (1999)', 'The Thing (1982)', 'Room (2015)', 'Gone With the Wind (1939)', 'Jurassic Park (1993)', 'Blade Runner (1982)', 'The Bridge on the River Kwai (1957)', 'Wild Strawberries (1957)', 'Finding Nemo (2003)', 'The Third Man (1949)', 'On the Waterfront (1954)', 'Stalker (1979)', 'Fargo (1996)', 'Kill Bill (2003)', 'The Truman Show (1998)', 'Gran Torino (2008)', 'Tokyo Story (1953)', 'The Deer Hunter (1978)', 'Memories of Murder (2003)', 'Wild Tales (2014)', 'Eskiya (1996)', 'Andhadhun (2018)', 'Klaus (2019)', 'The Big Lebowski (1998)', 'Mary and Max (2009)', 'In the Name of the Father (1993)', 'Gone Girl (2014)', 'Hacksaw Ridge (2016)', 'The Grand Budapest Hotel (2014)', \"Le Mans '66 (2019)\", 'Mr. Smith Goes to Washington (1939)', 'The General (1926)', 'Persona (1966)', 'How to Train Your Dragon (2010)', 'Before Sunrise (1995)', 'Catch Me If You Can (2002)', 'Sherlock Jr. (1924)', 'Prisoners (2013)', '12 Years a Slave (2013)', 'Cool Hand Luke (1967)', 'Mad Max: Fury Road (2015)', 'The Wages of Fear (1953)', 'Network (1976)', 'Stand By Me (1986)', 'Into the Wild (2007)', 'Barry Lyndon (1975)', 'Life of Brian (1979)', 'Million Dollar Baby (2004)', 'Platoon (1986)', \"Hachiko: a Dog's Story (2009)\", 'Ben-Hur (1959)', 'Rush (2013)', 'Logan (2017)', 'Andrei Rublev (1966)', 'The Passion of Joan of Arc (1928)', 'Harry Potter and the Deathly Hallows: Part II (2011)', 'Dead Poets Society (1989)', 'The 400 Blows (1959)', 'Paint it Saffron (2006)', 'Amores Perros (2000)', 'Hotel Rwanda (2004)', 'Nausicaä of the Valley of the Wind (1984)', 'Spotlight (2015)', 'The Handmaiden (2016)', 'Rebecca (1940)', 'Rocky (1976)', 'Portrait of a Lady on Fire (2019)', 'Monsters, Inc. (2001)', 'La Haine (1995)', 'It Happened One Night (1934)', 'In the Mood for Love (2000)', 'Gangs of Wasseypur (2012)', 'Before Sunset (2004)', 'The Princess Bride (1987)', 'Ace in the Hole (1951)', 'The Help (2011)', 'The Red Shoes (1948)', 'Paris, Texas (1984)', 'The Invisible Guest (2016)', 'The Terminator (1984)', 'Lagaan: Once Upon a Time in India (2001)', 'Drishyam (2015)', 'Butch Cassidy and the Sundance Kid (1969)', 'Akira (1988)', 'Aladdin (1992)', 'PK (2014)', 'Winter Sleep (2014)', 'A Silent Voice (2016)', 'Fanny Och Alexander (1982)']\n"
     ]
    }
   ],
   "source": [
    "# since movie title on IMDb would automatically convert my website title to chinese according to my web domain\n",
    "#so I need to check out ther website for movie titles\n",
    "url = \"http://filmcentrum.nl/imdb250/\"\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text, 'html.parser')\n",
    "titles = soup.find_all('div',{'class':\"lijstenFilm\"})\n",
    "years = soup.find_all(\"div\", {'class': \"lijstenJaar\"})\n",
    "process_title = []\n",
    "count = 0\n",
    "#print(titles)\n",
    "# remove blank in titles\n",
    "for i in titles:\n",
    "    ii = i.get_text()\n",
    "    process_title.append(ii[6:-2] + \" (\"+ str(years[count].get_text()) +\")\")\n",
    "    count += 1\n",
    "    \n",
    "print(process_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    376\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m                 \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-48b4f388c5f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlinks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://www.imdb.com\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"plotsummary?ref_=tt_stry_pl#synopsis\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'li'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'^synopsis-py'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    531\u001b[0m         }\n\u001b[0;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m                 )\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m                                                   chunked=chunked)\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m             \u001b[1;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1322\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Unexpected EOF'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\OpenSSL\\SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1819\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_peek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1820\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1821\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1822\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "url = \"https://www.imdb.com/chart/top/?ref_=nv_mv_250\"\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text, 'html.parser')\n",
    "links = soup.find_all(\"a\", href = re.compile(\"^/title/tt\\d\"))\n",
    "article = []\n",
    "#link = links\n",
    "for link in links[:200]:\n",
    "    url = \"https://www.imdb.com\" + str(link['href']) + \"plotsummary?ref_=tt_stry_pl#synopsis\"\n",
    "    html = requests.get(url)\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    text = soup.find('li', id=re.compile('^synopsis-py'))\n",
    "    # 'NoneType' object has no attribute 'get_text'\n",
    "    if text is None: \n",
    "        pass\n",
    "    else:\n",
    "        article.append(text.get_text())\n",
    "print(len(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk,os\n",
    "nltk.download('punkt')\n",
    "doc_all = {}\n",
    "i = 0\n",
    "#print(article[0])\n",
    "for title in process_title[:95]:\n",
    "    tokens = nltk.word_tokenize(article[i])\n",
    "    token_filtered = [w.lower() for w in tokens if w.isalpha()]\n",
    "    doc_all[title]=token_filtered\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert all document to vector, use tfidf weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tf(term, token_doc):\n",
    "    tf = token_doc.count(term)/len(token_doc)\n",
    "    return tf\n",
    "\n",
    "# create function to calculate how many doc contain the term \n",
    "def numDocsContaining(word, token_doclist):\n",
    "    doccount = 0\n",
    "    for doc_token in token_doclist:\n",
    "        if doc_token.count(word) > 0:\n",
    "            doccount +=1\n",
    "    return doccount\n",
    "  \n",
    "import math\n",
    "# create function to calculate  Inverse Document Frequency in doclist - this list of all documents\n",
    "def idf(word, token_doclist):\n",
    "    n = len(token_doclist)\n",
    "    df = numDocsContaining(word, token_doclist)\n",
    "    if df==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return math.log10(n/df)\n",
    "\n",
    "#define a function to do cosine normalization a data dictionary\n",
    "def cos_norm(dic): # dic is distionary data structure\n",
    "    dic_norm={}\n",
    "    factor=1.0/np.sqrt(sum([np.square(i) for i in dic.values()]))\n",
    "    for k in dic:\n",
    "        dic_norm[k] = dic[k]*factor\n",
    "    return dic_norm\n",
    "\n",
    "#create function to calculate normalize tfidf \n",
    "def compute_tfidf(token_doc,bag_words_idf):\n",
    "    tfidf_doc={}\n",
    "    for word in set(token_doc):\n",
    "        if word not in bag_words_idf.keys(): # may not find keys\n",
    "            pass\n",
    "        else:\n",
    "            tfidf_doc[word]= tf(word,token_doc) * bag_words_idf[word] \n",
    "    tfidf_norm = cos_norm(tfidf_doc)\n",
    "    return tfidf_norm\n",
    "\n",
    "# create normalize term frequency\n",
    "def tf_norm(token_doc):\n",
    "    tf_norm={}\n",
    "    for term in token_doc:\n",
    "        tf = token_doc.count(term)/len(token_doc)\n",
    "        tf_norm[term]=tf\n",
    "    tf_max = max(tf_norm.values())\n",
    "    for term, value in tf_norm.items():\n",
    "        tf_norm[term]= 0.5 + 0.5*value/tf_max\n",
    "    return tf_norm\n",
    "\n",
    "def compute_tfidf_query(query_token,bag_words_idf):\n",
    "    tfidf_query={}\n",
    "    tf_norm_query = tf_norm(query_token)\n",
    "    for term, value in tf_norm_query.items():\n",
    "        tfidf_query[term]=value*bag_words_idf[term]   \n",
    "    return tfidf_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "file = open(\"time_result.txt\",\"w\")#write mode \n",
    "now = datetime.now()\n",
    "start = now.strftime(\"%H:%M:%S\")\n",
    "file.write(\"-------------------No feature selection--------------------\")\n",
    "print(\"Start tfidf processing at\", start)\n",
    "file.write(\"\\nStart tfidf processing at \"+ str(start))\n",
    "\n",
    "#create bag words\n",
    "bag_words =[] # declare bag_words is a list\n",
    "for doc in doc_all.keys():\n",
    "    bag_words += doc_all[doc]\n",
    "bag_words=set(bag_words)\n",
    "\n",
    "#calculate idf for every word in bag_words\n",
    "bag_words_idf={}  \n",
    "bag_words_len = len(bag_words)\n",
    "bag_word_10 = round(bag_words_len/10,0)\n",
    "print(\"the number of term in bag_word\", bag_words_len)\n",
    "print(\"Start idf processing\")\n",
    "i=0\n",
    "for word in bag_words:\n",
    "    i+=1\n",
    "    if (i%bag_word_10==0):print(\"finish %s idf processing\" %(str(round(i*10/bag_word_10))+\"%\"))\n",
    "    bag_words_idf[word]= idf(word,doc_all.values())\n",
    "\n",
    "print(\"finish idf processing\")\n",
    "##calculate tfidf with cosine normalization\n",
    "tfidf={} # declare tfidf dictionary to store tfidf value\n",
    "for doc in doc_all.keys():\n",
    "    tfidf[doc]= compute_tfidf(doc_all[doc],bag_words_idf)\n",
    "    #print(tfidf[doc])\n",
    "print(\"finish tfidf processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input query text, and take similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"encourage wise hope\"\n",
    "query_token_raw= nltk.word_tokenize(query)\n",
    "query_token = [term for term in query_token_raw if term in bag_words]\n",
    "\n",
    "tfidf_query =compute_tfidf_query(query_token,bag_words_idf) #calculate tfidf for query text\n",
    "print(tfidf_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add tfidf of query text to tfidf of all doc and convert to dataframe\n",
    "tfidf[\"query\"] = tfidf_query\n",
    "\n",
    "import pandas as pd\n",
    "tfidf_df = pd.DataFrame(tfidf).transpose()\n",
    "tfidf_df= tfidf_df.fillna(0) # replace all NaN by zero\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "cosine_sim ={}\n",
    "for row in tfidf_df.index:\n",
    "    if row != \"query\":\n",
    "        cosine_sim[row]= 1-cosine(tfidf_df.loc[row],tfidf_df.loc[\"query\"])\n",
    "\n",
    "# the top 10 relevant document\n",
    "cosine_sim_top10 = dict(sorted(cosine_sim.items(), key=lambda item: item[1],reverse=True)[:10])\n",
    "print(cosine_sim_top10)\n",
    "\n",
    "now = datetime.now()\n",
    "finish = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Finish tfidf processing at\", finish)\n",
    "file.write(\"\\nFinish tfidf processing at \"+str(finish))\n",
    "spent = finish - start\n",
    "file.write(\"\\nTotal spent time: \"+str(spent))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot barchart\n",
    "import matplotlib.pyplot as plt\n",
    "data = cosine_sim_top10\n",
    "plt.barh(range(len(data)), list(data.values()), align='center', alpha=0.8)\n",
    "plt.yticks(range(len(data)), list(data.keys())) # label for y axis\n",
    "plt.xlabel('Smimilarity score')\n",
    "plt.ylabel('Course')\n",
    "\n",
    "# save graph\n",
    "path = r\"C:\\Users\\USER\\Desktop\\Text-Mining\\hw6\"\n",
    "plt.savefig(path+\"\\\\barchart.png\", bbox_inches='tight', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With feature selection process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "stopword = nltk.download('stopwords')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "tokens_no_stop = [word for word in bag_words if not word in stopwords.words()]\n",
    "pos_tokens = nltk.pos_tag(tokens_no_stop, tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_terms(tokens):\n",
    "    NVA_tokens =[] \n",
    "    for token in tokens:\n",
    "        if token[1] == \"NOUN\":\n",
    "            NVA_tokens += [token[0]]\n",
    "        elif token[1] == \"VERB\":\n",
    "            NVA_tokens += [token[0]]\n",
    "        elif token[1] == \"ADJ\":\n",
    "            NVA_tokens += [token[0]]\n",
    "    return NVA_tokens\n",
    "\n",
    "new_tokens = extract_terms(pos_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "movies_porter = []\n",
    "for words in new_tokens:\n",
    "    movies_porter.append(ps.stem(words))\n",
    "    \n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"maxent_treebank_pos_tagger\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def lemmatize_sentence(words):\n",
    "    res = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for word, pos in pos_tag(words):\n",
    "        wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "        res.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "\n",
    "    return res\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "lem_and_porter = lemmatize_sentence(movies_porter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "freq_dist = FreqDist(lem_and_porter)\n",
    "print(freq_dist.most_common(10))\n",
    "print(\"Before extract:\",len(bag_words))\n",
    "print(\"After extract:\",len(new_tokens))\n",
    "print(len(lem_and_porter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate idf for every word in bag_words\n",
    "file = open(\"time_result.txt\",\"a\")#write mode \n",
    "now = datetime.now()\n",
    "start = now.strftime(\"%H:%M:%S\")\n",
    "file.write(\"\\n-------------------With feature selection--------------------\")\n",
    "print(\"Start tfidf processing at\", start)\n",
    "file.write(\"\\nStart tfidf processing at \"+ str(start))\n",
    "\n",
    "new_tokens = set(lem_and_porter)\n",
    "bag_words_idf={}  \n",
    "bag_words_len = len(new_tokens)\n",
    "bag_word_10 = round(bag_words_len/10,0)\n",
    "print(\"the number of term after doing feature selection\", bag_words_len)\n",
    "print(\"Start idf processing\")\n",
    "i=0\n",
    "for word in new_tokens:\n",
    "    i+=1\n",
    "    if (i%bag_word_10==0):print(\"finish %s idf processing\" %(str(round(i*10/bag_word_10))+\"%\"))\n",
    "    bag_words_idf[word]= idf(word,doc_all.values())\n",
    "\n",
    "print(\"finish idf processing\")\n",
    "##calculate tfidf with cosine normalization\n",
    "tfidf={} # declare tfidf dictionary to store tfidf value\n",
    "for doc in doc_all.keys():\n",
    "    tfidf[doc]= compute_tfidf(doc_all[doc],bag_words_idf)\n",
    "    #print(tfidf[doc])\n",
    "print(\"finish tfidf processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf[\"query\"] = tfidf_query\n",
    "tfidf_df = pd.DataFrame(tfidf).transpose()\n",
    "tfidf_df= tfidf_df.fillna(0) # replace all NaN by zero\n",
    "\n",
    "cosine_sim ={}\n",
    "for row in tfidf_df.index:\n",
    "    if row != \"query\":\n",
    "        cosine_sim[row]= 1-cosine(tfidf_df.loc[row],tfidf_df.loc[\"query\"])\n",
    "\n",
    "# the top 10 relevant document\n",
    "cosine_sim_top10 = dict(sorted(cosine_sim.items(), key=lambda item: item[1],reverse=True)[:10])\n",
    "print(cosine_sim_top10)\n",
    "\n",
    "now = datetime.now()\n",
    "finish = now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "print(\"\\nFinish tfidf processing at\", finish)\n",
    "file.write(\"\\nFinish tfidf processing at \" + str(finish))\n",
    "spent = finish - start\n",
    "file.write(\"\\nTotal spent time: \"+str(spent))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = cosine_sim_top10\n",
    "plt.barh(range(len(data)), list(data.values()), align='center', alpha=0.8)\n",
    "plt.yticks(range(len(data)), list(data.keys())) # label for y axis\n",
    "plt.xlabel('Smimilarity score')\n",
    "plt.ylabel('Course')\n",
    "\n",
    "# save graph\n",
    "path = r\"C:\\Users\\USER\\Desktop\\Text-Mining\\hw6\"\n",
    "plt.savefig(path+\"\\\\barchart_with_Feature_Selection.png\", bbox_inches='tight', dpi=600)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
