{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in c:\\users\\user\\anaconda3\\lib\\site-packages (3.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from gdown) (3.0.10)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\user\\anaconda3\\lib\\site-packages (from gdown) (2.21.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from gdown) (4.43.0)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from gdown) (1.12.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (1.6.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1jbb0HQ9oTZkRNK055AvXD3ya8_ZLtN9o\n",
      "To: C:\\Users\\USER\\Desktop\\Text-Mining\\hw7\\Amazon_review.csv\n",
      "4.52MB [00:00, 21.3MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Amazon_review.csv'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install gdown\n",
    "import gdown\n",
    "url = \"https://drive.google.com/uc?id=1jbb0HQ9oTZkRNK055AvXD3ya8_ZLtN9o\"\n",
    "gdown.download(url, \"Amazon_review.csv\", quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>an absolute masterpiece: I am quite sure any o...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Buyer beware: This is a self-published book, a...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Glorious story: I loved Whisper of the wicked ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A FIVE STAR BOOK: I just finished reading Whis...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Whispers of the Wicked Saints: This was a easy...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The Worst!: A complete waste of time. Typograp...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Great book: This was a great book,I just could...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Great Read: I thought this book was brilliant,...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Oh please: I guess you have to be a romance no...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Awful beyond belief!: I feel I have to write t...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Don't try to fool us with fake reviews.: It's ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A romantic zen baseball comedy: When you hear ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Fashionable Compression Stockings!: After I ha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Jobst UltraSheer Thigh High: Excellent product...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sizes recomended in the size chart are not rea...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>mens ultrasheer: This model may be ok for sede...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Delicious cookie mix: I thought it was funny t...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Another Abysmal Digital Copy: Rather than scra...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>A fascinating insight into the life of modern ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>i liked this album more then i thought i would...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Problem with charging smaller AAAs: I have had...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Works, but not as advertised: I bought one of ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Disappointed: I read the reviews,made my purch...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Oh dear: I was excited to find a book ostensib...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Based on the reviews here I bought one and I'm...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>beware: The product does give the surround sou...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>happy i only wasted money for 2 Music Experien...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>oh my goodness!: If this is a single release, ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>The dummy \"FATS\" is hysterical!!!!: ALL I can ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>Dummy Scared the Be-Jesus Out of Me: Oh, God, ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>More Ham-O-Rama Theatrics From Sir Anthony: Wh...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>Take The Knife Up The Hill And Rent This Movie...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>MAGIC ADS WERE SCARY!: Though the movie was fr...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>Deliciously disturbing ....Highly Underestimat...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>Magic: If you like Anthony Hopkins, this is on...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>Magic, on Blu Ray, starrring Anthony Hopkins a...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>A ventriloquists nightmare: Magic is a timeles...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>great movie massacred by tape quality: One of ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>Early Hopkins story still sends chills through...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>The Only Dummy Is The Writer: \"Magic\" poses th...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>\"He's NO Dummy. . .\": Viewing \"Magic\" is when ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>Amazingly suspenseful psychological thriller: ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>A truly great horror movie: I saw this film la...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>Frightening movie with superb acting by Sir Ho...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>classic: i got this for my dad. it is super cr...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>Psychological thriller!: This movie really sca...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>A little more money than what I expected to sp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>\"The Silence of the Dummies\": This is overall ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>Mauled again - killing bears to enrich himself...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>Sorry Jim: As a former realtor, Mr. Cole owes ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>A revelation of life in small town America in ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Great biography of a very interesting journali...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Interesting Subject; Poor Presentation: You'd ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Don't buy: The box looked used and it is obvio...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Beautiful Pen and Fast Delivery.: The pen was ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     label\n",
       "0     Stuning even for the non-gamer: This sound tra...  Positive\n",
       "1     The best soundtrack ever to anything.: I'm rea...  Positive\n",
       "2     Amazing!: This soundtrack is my favorite music...  Positive\n",
       "3     Excellent Soundtrack: I truly like this soundt...  Positive\n",
       "4     Remember, Pull Your Jaw Off The Floor After He...  Positive\n",
       "5     an absolute masterpiece: I am quite sure any o...  Positive\n",
       "6     Buyer beware: This is a self-published book, a...  Negative\n",
       "7     Glorious story: I loved Whisper of the wicked ...  Positive\n",
       "8     A FIVE STAR BOOK: I just finished reading Whis...  Positive\n",
       "9     Whispers of the Wicked Saints: This was a easy...  Positive\n",
       "10    The Worst!: A complete waste of time. Typograp...  Negative\n",
       "11    Great book: This was a great book,I just could...  Positive\n",
       "12    Great Read: I thought this book was brilliant,...  Positive\n",
       "13    Oh please: I guess you have to be a romance no...  Negative\n",
       "14    Awful beyond belief!: I feel I have to write t...  Negative\n",
       "15    Don't try to fool us with fake reviews.: It's ...  Negative\n",
       "16    A romantic zen baseball comedy: When you hear ...  Positive\n",
       "17    Fashionable Compression Stockings!: After I ha...  Positive\n",
       "18    Jobst UltraSheer Thigh High: Excellent product...  Positive\n",
       "19    sizes recomended in the size chart are not rea...  Negative\n",
       "20    mens ultrasheer: This model may be ok for sede...  Negative\n",
       "21    Delicious cookie mix: I thought it was funny t...  Positive\n",
       "22    Another Abysmal Digital Copy: Rather than scra...  Negative\n",
       "23    A fascinating insight into the life of modern ...  Positive\n",
       "24    i liked this album more then i thought i would...  Positive\n",
       "25    Problem with charging smaller AAAs: I have had...  Negative\n",
       "26    Works, but not as advertised: I bought one of ...  Negative\n",
       "27    Disappointed: I read the reviews,made my purch...  Negative\n",
       "28    Oh dear: I was excited to find a book ostensib...  Negative\n",
       "29    Based on the reviews here I bought one and I'm...  Positive\n",
       "...                                                 ...       ...\n",
       "9970  beware: The product does give the surround sou...  Negative\n",
       "9971  happy i only wasted money for 2 Music Experien...  Negative\n",
       "9972  oh my goodness!: If this is a single release, ...  Negative\n",
       "9973  The dummy \"FATS\" is hysterical!!!!: ALL I can ...  Positive\n",
       "9974  Dummy Scared the Be-Jesus Out of Me: Oh, God, ...  Negative\n",
       "9975  More Ham-O-Rama Theatrics From Sir Anthony: Wh...  Negative\n",
       "9976  Take The Knife Up The Hill And Rent This Movie...  Positive\n",
       "9977  MAGIC ADS WERE SCARY!: Though the movie was fr...  Positive\n",
       "9978  Deliciously disturbing ....Highly Underestimat...  Positive\n",
       "9979  Magic: If you like Anthony Hopkins, this is on...  Positive\n",
       "9980  Magic, on Blu Ray, starrring Anthony Hopkins a...  Positive\n",
       "9981  A ventriloquists nightmare: Magic is a timeles...  Positive\n",
       "9982  great movie massacred by tape quality: One of ...  Negative\n",
       "9983  Early Hopkins story still sends chills through...  Positive\n",
       "9984  The Only Dummy Is The Writer: \"Magic\" poses th...  Negative\n",
       "9985  \"He's NO Dummy. . .\": Viewing \"Magic\" is when ...  Positive\n",
       "9986  Amazingly suspenseful psychological thriller: ...  Positive\n",
       "9987  A truly great horror movie: I saw this film la...  Positive\n",
       "9988  Frightening movie with superb acting by Sir Ho...  Positive\n",
       "9989  classic: i got this for my dad. it is super cr...  Positive\n",
       "9990  Psychological thriller!: This movie really sca...  Positive\n",
       "9991  A little more money than what I expected to sp...  Positive\n",
       "9992  \"The Silence of the Dummies\": This is overall ...  Negative\n",
       "9993  Mauled again - killing bears to enrich himself...  Negative\n",
       "9994  Sorry Jim: As a former realtor, Mr. Cole owes ...  Negative\n",
       "9995  A revelation of life in small town America in ...  Positive\n",
       "9996  Great biography of a very interesting journali...  Positive\n",
       "9997  Interesting Subject; Poor Presentation: You'd ...  Negative\n",
       "9998  Don't buy: The box looked used and it is obvio...  Negative\n",
       "9999  Beautiful Pen and Fast Delivery.: The pen was ...  Positive\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "amazon_reviewDF = pd.read_csv(\"Amazon_review.csv\",delimiter=',',header = None)\n",
    "amazon_reviewDF.columns=['text', 'label'] # name the column of dataframe \n",
    "amazon_reviewDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "def spacy_preprocess(text,lemma= True, pos= True, pos_select = [\"VERB\", \"NOUN\", \"ADJ\",\"ADV\",\"PART\"]):\n",
    "    # Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner']) # disable parser, ner for faster loading\n",
    "    # Parse the sentence using the loaded 'en' model object `nlp`\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    if pos== False:\n",
    "        if lemma== True: text_preprocess= \" \".join([token.lemma_ for token in doc])\n",
    "        if lemma== False:text_preprocess= \" \".join([token.text for token in doc])\n",
    "    else:\n",
    "        if lemma== True : text_preprocess= \" \".join([token.lemma_ for token in doc if token.pos_ in pos_select])\n",
    "        if lemma== False : text_preprocess= \" \".join([token.text for token in doc if token.pos_ in pos_select])\n",
    "    return text_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 100.0%\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "#this may take long time to process all 10.000 comments\n",
    "amazon_reviewDF_lem_pos = amazon_reviewDF\n",
    "for index in amazon_reviewDF.index:\n",
    "    text = amazon_reviewDF['text'][index]\n",
    "    amazon_reviewDF_lem_pos['text'][index]= spacy_preprocess (text,lemma= True, pos= True, pos_select = [\"VERB\", \"ADJ\",\"ADV\"])\n",
    "    # print out the perentage complete\n",
    "    clear_output(wait=True)\n",
    "    if ((index+1)%10==0):print(\"finish %s\" %(str(round((index+1)/100,2))+\"%\"))\n",
    "\n",
    "# save the result after remove pos and lemmatizaiton to use later\n",
    "amazon_reviewDF_lem_pos.to_csv(\"Amazon_review_lem_pos.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_reviewDF_lem_pos = pd.read_csv(\"Amazon_review_lem_pos.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we have 2 data frame:**\n",
    "\n",
    "the original one: amazon_reviewDF <br>\n",
    "the data after lemmatization, pos: amazon_reviewDF_lem_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Test 1: Does using lemmatization and POS selection give a better result?**\n",
    "\n",
    "No Lemmatization & POS result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "# split the dataset into training and testing \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(amazon_reviewDF['text'], \n",
    "                                                      amazon_reviewDF['label'],test_size=0.1,random_state=1)\n",
    "\n",
    "#test_size = 0.1 mean 90% data is used for traning data, 10% for testing data.\n",
    "\n",
    "from sklearn import preprocessing\n",
    "# encode the target variable, label Negative/Positive -> 0/1\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y_en = encoder.fit_transform(train_y)\n",
    "valid_y_en = encoder.fit_transform(valid_y)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english',  max_features = None)\n",
    "tfidf_vect.fit(train_x.values.astype('U')) #Learn vocabulary and idf from training data set. (in bad words)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x.values.astype('U')) # compute tfidf for tranning data set\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x.values.astype('U')) # compute tfidf for testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.702\n",
      "F-score 0.7014028056112225\n",
      "confusion_matrix\n",
      " [[352 158]\n",
      " [140 350]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree, metrics\n",
    "classifier_tree = tree.DecisionTreeClassifier(random_state=1)\n",
    "# fit the training dataset on the classifier\n",
    "classifier_tree =classifier_tree.fit(xtrain_tfidf, train_y_en)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "valid_y_pred_tree = classifier_tree.predict(xvalid_tfidf)\n",
    "accuracy_tree = metrics.accuracy_score(valid_y_en, valid_y_pred_tree)\n",
    "confusion_matrix_tree = metrics.confusion_matrix(valid_y_en, valid_y_pred_tree, labels=[0,1])\n",
    "print (\"Accuracy\", accuracy_tree)\n",
    "F1_score1=metrics.f1_score(valid_y_en, valid_y_pred_tree)\n",
    "print (\"F-score\", F1_score1)\n",
    "print (\"confusion_matrix\\n\", confusion_matrix_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Lemmatization, POS result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(amazon_reviewDF_lem_pos['text'], \n",
    "                                                      amazon_reviewDF_lem_pos['label'],test_size=0.1,random_state=1)\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y_en = encoder.fit_transform(train_y)\n",
    "valid_y_en = encoder.fit_transform(valid_y)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english',  max_features = None)\n",
    "tfidf_vect.fit(train_x.values.astype('U')) #Learn vocabulary and idf from training data set. (in bad words)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x.values.astype('U')) # compute tfidf for tranning data set\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x.values.astype('U')) # compute tfidf for testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.711\n",
      "F-score 0.7107107107107107\n",
      "confusion_matrix\n",
      " [[356 154]\n",
      " [135 355]]\n"
     ]
    }
   ],
   "source": [
    "classifier_tree = tree.DecisionTreeClassifier(random_state=1)\n",
    "# fit the training dataset on the classifier\n",
    "classifier_tree =classifier_tree.fit(xtrain_tfidf, train_y_en)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "valid_y_pred_tree = classifier_tree.predict(xvalid_tfidf)\n",
    "accuracy_tree = metrics.accuracy_score(valid_y_en, valid_y_pred_tree)\n",
    "confusion_matrix_tree = metrics.confusion_matrix(valid_y_en, valid_y_pred_tree, labels=[0,1])\n",
    "print (\"Accuracy\", accuracy_tree)\n",
    "F1_score1=metrics.f1_score(valid_y_en, valid_y_pred_tree)\n",
    "print (\"F-score\", F1_score1)\n",
    "print (\"confusion_matrix\\n\", confusion_matrix_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with Lemmatization & POS has better performance.\n",
    "So we gonna use this model for later experiments.\n",
    "\n",
    "**(b) Test 2: if we limit the max_fetaure, does it affect the model performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max_feature: 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(amazon_reviewDF_lem_pos['text'], \n",
    "                                                      amazon_reviewDF_lem_pos['label'],test_size=0.1,random_state=1)\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y_en = encoder.fit_transform(train_y)\n",
    "valid_y_en = encoder.fit_transform(valid_y)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english', max_features = 1500)\n",
    "tfidf_vect.fit(train_x.values.astype('U')) #Learn vocabulary and idf from training data set. (in bad words)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x.values.astype('U')) # compute tfidf for tranning data set\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x.values.astype('U')) # compute tfidf for testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.71\n",
      "F-score 0.7140039447731755\n",
      "confusion_matrix\n",
      " [[348 162]\n",
      " [128 362]]\n"
     ]
    }
   ],
   "source": [
    "classifier_tree = tree.DecisionTreeClassifier(random_state=1)\n",
    "# fit the training dataset on the classifier\n",
    "classifier_tree =classifier_tree.fit(xtrain_tfidf, train_y_en)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "valid_y_pred_tree = classifier_tree.predict(xvalid_tfidf)\n",
    "accuracy_tree = metrics.accuracy_score(valid_y_en, valid_y_pred_tree)\n",
    "confusion_matrix_tree = metrics.confusion_matrix(valid_y_en, valid_y_pred_tree, labels=[0,1])\n",
    "print (\"Accuracy\", accuracy_tree)\n",
    "F1_score1=metrics.f1_score(valid_y_en, valid_y_pred_tree)\n",
    "print (\"F-score\", F1_score1)\n",
    "print (\"confusion_matrix\\n\", confusion_matrix_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max_feature: 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(stop_words='english',  max_features = 2500)\n",
    "tfidf_vect.fit(train_x.values.astype('U')) #Learn vocabulary and idf from training data set. (in bad words)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x.values.astype('U')) # compute tfidf for tranning data set\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x.values.astype('U')) # compute tfidf for testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.701\n",
      "F-score 0.7007007007007007\n",
      "confusion_matrix\n",
      " [[351 159]\n",
      " [140 350]]\n"
     ]
    }
   ],
   "source": [
    "classifier_tree = tree.DecisionTreeClassifier(random_state=1)\n",
    "# fit the training dataset on the classifier\n",
    "classifier_tree =classifier_tree.fit(xtrain_tfidf, train_y_en)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "valid_y_pred_tree = classifier_tree.predict(xvalid_tfidf)\n",
    "accuracy_tree = metrics.accuracy_score(valid_y_en, valid_y_pred_tree)\n",
    "confusion_matrix_tree = metrics.confusion_matrix(valid_y_en, valid_y_pred_tree, labels=[0,1])\n",
    "print (\"Accuracy\", accuracy_tree)\n",
    "F1_score1=metrics.f1_score(valid_y_en, valid_y_pred_tree)\n",
    "print (\"F-score\", F1_score1)\n",
    "print (\"confusion_matrix\\n\", confusion_matrix_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max_feature: 3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(stop_words='english',  max_features = 3500)\n",
    "tfidf_vect.fit(train_x.values.astype('U')) #Learn vocabulary and idf from training data set. (in bad words)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x.values.astype('U')) # compute tfidf for tranning data set\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x.values.astype('U')) # compute tfidf for testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.721\n",
      "F-score 0.716751269035533\n",
      "confusion_matrix\n",
      " [[368 142]\n",
      " [137 353]]\n"
     ]
    }
   ],
   "source": [
    "classifier_tree = tree.DecisionTreeClassifier(random_state=1)\n",
    "# fit the training dataset on the classifier\n",
    "classifier_tree =classifier_tree.fit(xtrain_tfidf, train_y_en)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "valid_y_pred_tree = classifier_tree.predict(xvalid_tfidf)\n",
    "accuracy_tree = metrics.accuracy_score(valid_y_en, valid_y_pred_tree)\n",
    "confusion_matrix_tree = metrics.confusion_matrix(valid_y_en, valid_y_pred_tree, labels=[0,1])\n",
    "print (\"Accuracy\", accuracy_tree)\n",
    "F1_score1=metrics.f1_score(valid_y_en, valid_y_pred_tree)\n",
    "print (\"F-score\", F1_score1)\n",
    "print (\"confusion_matrix\\n\", confusion_matrix_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max_feature: 4300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(stop_words='english',  max_features = 4300)\n",
    "tfidf_vect.fit(train_x.values.astype('U')) #Learn vocabulary and idf from training data set. (in bad words)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x.values.astype('U')) # compute tfidf for tranning data set\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x.values.astype('U')) # compute tfidf for testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.729\n",
      "F-score 0.7292707292707294\n",
      "confusion_matrix\n",
      " [[364 146]\n",
      " [125 365]]\n"
     ]
    }
   ],
   "source": [
    "classifier_tree = tree.DecisionTreeClassifier(random_state=1)\n",
    "# fit the training dataset on the classifier\n",
    "classifier_tree =classifier_tree.fit(xtrain_tfidf, train_y_en)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "valid_y_pred_tree = classifier_tree.predict(xvalid_tfidf)\n",
    "accuracy_tree = metrics.accuracy_score(valid_y_en, valid_y_pred_tree)\n",
    "confusion_matrix_tree = metrics.confusion_matrix(valid_y_en, valid_y_pred_tree, labels=[0,1])\n",
    "print (\"Accuracy\", accuracy_tree)\n",
    "F1_score1=metrics.f1_score(valid_y_en, valid_y_pred_tree)\n",
    "print (\"F-score\", F1_score1)\n",
    "print (\"confusion_matrix\\n\", confusion_matrix_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max_feature: 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(stop_words='english',  max_features = 5000)\n",
    "tfidf_vect.fit(train_x.values.astype('U')) #Learn vocabulary and idf from training data set. (in bad words)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x.values.astype('U')) # compute tfidf for tranning data set\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x.values.astype('U')) # compute tfidf for testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.715\n",
      "F-score 0.7064881565396498\n",
      "confusion_matrix\n",
      " [[372 138]\n",
      " [147 343]]\n"
     ]
    }
   ],
   "source": [
    "classifier_tree = tree.DecisionTreeClassifier(random_state=1)\n",
    "# fit the training dataset on the classifier\n",
    "classifier_tree =classifier_tree.fit(xtrain_tfidf, train_y_en)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "valid_y_pred_tree = classifier_tree.predict(xvalid_tfidf)\n",
    "accuracy_tree = metrics.accuracy_score(valid_y_en, valid_y_pred_tree)\n",
    "confusion_matrix_tree = metrics.confusion_matrix(valid_y_en, valid_y_pred_tree, labels=[0,1])\n",
    "print (\"Accuracy\", accuracy_tree)\n",
    "F1_score1=metrics.f1_score(valid_y_en, valid_y_pred_tree)\n",
    "print (\"F-score\", F1_score1)\n",
    "print (\"confusion_matrix\\n\", confusion_matrix_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that when the max_features=4300 , the accuracy is the best, so we decide to choose it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Text 3: Use different classification algorithm:**\n",
    "\n",
    "**Use DecisionTreeClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(amazon_reviewDF_lem_pos['text'], \n",
    "                                                      amazon_reviewDF_lem_pos['label'],test_size=0.1,random_state=1)\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y_en = encoder.fit_transform(train_y)\n",
    "valid_y_en = encoder.fit_transform(valid_y)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english',  max_features = 4300) # with best feature size\n",
    "tfidf_vect.fit(train_x.values.astype('U')) #Learn vocabulary and idf from training data set. (in bad words)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x.values.astype('U')) # compute tfidf for tranning data set\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x.values.astype('U')) # compute tfidf for testing data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.729\n",
      "F-score 0.7292707292707294\n",
      "confusion_matrix\n",
      " [[364 146]\n",
      " [125 365]]\n"
     ]
    }
   ],
   "source": [
    "classifier_tree = tree.DecisionTreeClassifier(random_state=1)\n",
    "# fit the training dataset on the classifier\n",
    "classifier_tree =classifier_tree.fit(xtrain_tfidf, train_y_en)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "valid_y_pred_tree = classifier_tree.predict(xvalid_tfidf)\n",
    "accuracy_tree = metrics.accuracy_score(valid_y_en, valid_y_pred_tree)\n",
    "confusion_matrix_tree = metrics.confusion_matrix(valid_y_en, valid_y_pred_tree, labels=[0,1])\n",
    "print (\"Accuracy\", accuracy_tree)\n",
    "F1_score1=metrics.f1_score(valid_y_en, valid_y_pred_tree)\n",
    "print (\"F-score\", F1_score1)\n",
    "print (\"confusion_matrix\\n\", confusion_matrix_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use RandomForestClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.808\n",
      "F-score 0.8040816326530612\n",
      "confusion_matrix\n",
      " [[414  96]\n",
      " [ 96 394]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "classifier_rf = ensemble.RandomForestClassifier(random_state= 1)\n",
    "# fit the training dataset on the classifier\n",
    "classifier_rf.fit(xtrain_tfidf, train_y_en)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "valid_y_pred_rf = classifier_rf.predict(xvalid_tfidf)\n",
    "\n",
    "accuracy_rf = metrics.accuracy_score(valid_y_en, valid_y_pred_rf)\n",
    "confusion_matrix_rf = metrics.confusion_matrix(valid_y_en, valid_y_pred_rf, labels=[0,1])\n",
    "F1_score=metrics.f1_score(valid_y_en, valid_y_pred_rf)\n",
    "print (\"Accuracy\", accuracy_rf)\n",
    "print (\"F-score\", F1_score)\n",
    "print (\"confusion_matrix\\n\", confusion_matrix_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Neural Network Multi-layer Perceptron classifier. (default setting)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.805\n",
      "F-score 0.7991761071060762\n",
      "confusion_matrix\n",
      " [[417  93]\n",
      " [102 388]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "classifier_nn = MLPClassifier(random_state = 1)\n",
    "# fit the training dataset on the classifier\n",
    "classifier_nn.fit(xtrain_tfidf, train_y_en) # take around 3-4 minutes\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "valid_y_pred_nn = classifier_nn.predict(xvalid_tfidf)\n",
    "\n",
    "accuracy_nn = metrics.accuracy_score(valid_y_en, valid_y_pred_nn)\n",
    "confusion_matrix_nn = metrics.confusion_matrix(valid_y_en, valid_y_pred_nn, labels=[0,1])\n",
    "print (\"Accuracy\", accuracy_nn)\n",
    "F1_score=metrics.f1_score(valid_y_en, valid_y_pred_nn)\n",
    "print (\"F-score\", F1_score)\n",
    "print (\"confusion_matrix\\n\", confusion_matrix_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network Multi-layer Perceptron classifier. (user setting)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.69367271\n",
      "Iteration 2, loss = 0.68910346\n",
      "Iteration 3, loss = 0.68216429\n",
      "Iteration 4, loss = 0.66515741\n",
      "Iteration 5, loss = 0.62331110\n",
      "Iteration 6, loss = 0.54009972\n",
      "Iteration 7, loss = 0.45134447\n",
      "Iteration 8, loss = 0.39497410\n",
      "Iteration 9, loss = 0.36023305\n",
      "Iteration 10, loss = 0.33507451\n",
      "Accuracy 0.846\n",
      "F-score 0.8415637860082303\n",
      "confusion_matrix\n",
      " [[437  73]\n",
      " [ 81 409]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "classifier_nn = MLPClassifier(solver='sgd', activation='relu',alpha=1e-4,hidden_layer_sizes=(50,50), \n",
    "                              random_state=1,max_iter=10,verbose=10,learning_rate_init=.03)\n",
    "# fit the training dataset on the classifier\n",
    "classifier_nn.fit(xtrain_tfidf, train_y_en) # take around 3-4 minutes\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "valid_y_pred_nn = classifier_nn.predict(xvalid_tfidf)\n",
    "\n",
    "accuracy_nn = metrics.accuracy_score(valid_y_en, valid_y_pred_nn)\n",
    "confusion_matrix_nn = metrics.confusion_matrix(valid_y_en, valid_y_pred_nn, labels=[0,1])\n",
    "print (\"Accuracy\", accuracy_nn)\n",
    "F1_score=metrics.f1_score(valid_y_en, valid_y_pred_nn)\n",
    "print (\"F-score\", F1_score)\n",
    "print (\"confusion_matrix\\n\", confusion_matrix_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use logisticRegression Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.837\n",
      "F-score 0.8328205128205128\n",
      "confusion_matrix\n",
      " [[431  79]\n",
      " [ 84 406]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(random_state = 1)\n",
    "# fit the training dataset on the classifier\n",
    "model.fit(xtrain_tfidf, train_y_en) # take around 3-4 minutes\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "valid_y_pred_nn = model.predict(xvalid_tfidf)\n",
    "\n",
    "accuracy_nn = metrics.accuracy_score(valid_y_en, valid_y_pred_nn)\n",
    "confusion_matrix_nn = metrics.confusion_matrix(valid_y_en, valid_y_pred_nn, labels=[0,1])\n",
    "print (\"Accuracy\", accuracy_nn)\n",
    "F1_score=metrics.f1_score(valid_y_en, valid_y_pred_nn)\n",
    "print (\"F-score\", F1_score)\n",
    "print (\"confusion_matrix\\n\", confusion_matrix_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use Adaboost Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.815\n",
      "F-score 0.8102564102564104\n",
      "confusion_matrix\n",
      " [[420  90]\n",
      " [ 95 395]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model2 = AdaBoostClassifier(n_estimators=400, learning_rate= 0.4, random_state= 1) # 0.27\n",
    "# fit the training dataset on the classifier\n",
    "model2.fit(xtrain_tfidf, train_y_en) # take around 3-4 minutes\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "valid_y_pred_nn = model2.predict(xvalid_tfidf)\n",
    "\n",
    "accuracy_nn = metrics.accuracy_score(valid_y_en, valid_y_pred_nn)\n",
    "confusion_matrix_nn = metrics.confusion_matrix(valid_y_en, valid_y_pred_nn, labels=[0,1])\n",
    "print (\"Accuracy\", accuracy_nn)\n",
    "F1_score=metrics.f1_score(valid_y_en, valid_y_pred_nn)\n",
    "print (\"F-score\", F1_score)\n",
    "print (\"confusion_matrix\\n\", confusion_matrix_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that the result of Neural Network Multi-layer Perceptron with our tuning parameters is the best.** \n",
    "<br>\n",
    "\n",
    "Performance ranks are:\n",
    "1. Neural Network Multi-layer Perceptron with parameters tuning.\n",
    "2. Logistic \n",
    "3. Adaboost\n",
    "4. RandomForest\n",
    "5. Neural Network Multi-layer Perceptron original\n",
    "6. DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
