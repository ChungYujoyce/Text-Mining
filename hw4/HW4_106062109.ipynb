{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4- POS and vectorize plain text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization & POS tagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Read 3 document (doc1.txt, doc2.txt, doc3.txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "file1 = read_data('./data/doc1.txt')\n",
    "file2 = read_data('./data/doc2.txt')\n",
    "file3 = read_data('./data/doc3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Normalize words: convert all upper-case letter to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to get started, the python sections are linked at the left -- python set up to get python installed on your machine, python introduction for an introduction to the language, and then python strings starts the coding material, leading to the first exercise. the end of each written section includes a link to the code exercise for that section's material. the lecture videos parallel the written materials, introducing python, then strings, then first exercises, and so on. at google, all this material makes up an intensive 2-day class, so the videos are organized as the day-1 and day-2 sections.\n",
      "\n",
      "this material was created by nick parlante working in the engedu group at google. special thanks for the help from my google colleagues john cox, steve glassman, piotr kaminksi, and antoine picard. and finally thanks to google and my director maggie johnson for the enlightened generosity to put these materials out on the internet for free under the creative commons attribution 2.5 license -- share and enjoy!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def normalized(file):\n",
    "    file = file.lower()\n",
    "    return file\n",
    "\n",
    "norm_file1 = normalized(file1)\n",
    "norm_file2 = normalized(file2)\n",
    "norm_file3 = normalized(file3)\n",
    "print(norm_file1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'get', 'started', ',', 'the', 'python', 'sections', 'are', 'linked', 'at', 'the', 'left', '--', 'python', 'set', 'up', 'to', 'get', 'python', 'installed', 'on', 'your', 'machine', ',', 'python', 'introduction', 'for', 'an', 'introduction', 'to', 'the', 'language', ',', 'and', 'then', 'python', 'strings', 'starts', 'the', 'coding', 'material', ',', 'leading', 'to', 'the', 'first', 'exercise', '.', 'the', 'end', 'of', 'each', 'written', 'section', 'includes', 'a', 'link', 'to', 'the', 'code', 'exercise', 'for', 'that', 'section', \"'s\", 'material', '.', 'the', 'lecture', 'videos', 'parallel', 'the', 'written', 'materials', ',', 'introducing', 'python', ',', 'then', 'strings', ',', 'then', 'first', 'exercises', ',', 'and', 'so', 'on', '.', 'at', 'google', ',', 'all', 'this', 'material', 'makes', 'up', 'an', 'intensive', '2-day', 'class', ',', 'so', 'the', 'videos', 'are', 'organized', 'as', 'the', 'day-1', 'and', 'day-2', 'sections', '.', 'this', 'material', 'was', 'created', 'by', 'nick', 'parlante', 'working', 'in', 'the', 'engedu', 'group', 'at', 'google', '.', 'special', 'thanks', 'for', 'the', 'help', 'from', 'my', 'google', 'colleagues', 'john', 'cox', ',', 'steve', 'glassman', ',', 'piotr', 'kaminksi', ',', 'and', 'antoine', 'picard', '.', 'and', 'finally', 'thanks', 'to', 'google', 'and', 'my', 'director', 'maggie', 'johnson', 'for', 'the', 'enlightened', 'generosity', 'to', 'put', 'these', 'materials', 'out', 'on', 'the', 'internet', 'for', 'free', 'under', 'the', 'creative', 'commons', 'attribution', '2.5', 'license', '--', 'share', 'and', 'enjoy', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "token1 = nltk.word_tokenize(norm_file1)\n",
    "token2 = nltk.word_tokenize(norm_file2)\n",
    "token3 = nltk.word_tokenize(norm_file3)\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Remove stopwords using stopwords from nltk or other sources and remove punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'started', ',', 'python', 'sections', 'linked', 'left', '--', 'python', 'set', 'get', 'python', 'installed', 'machine', ',', 'python', 'introduction', 'introduction', 'language', ',', 'python', 'strings', 'starts', 'coding', 'material', ',', 'leading', 'first', 'exercise', '.', 'written', 'section', 'includes', 'link', 'code', 'exercise', 'section', \"'s\", 'material', '.', 'lecture', 'videos', 'parallel', 'written', 'materials', ',', 'introducing', 'python', ',', 'strings', ',', 'first', 'exercises', ',', '.', 'google', ',', 'material', 'makes', 'intensive', '2-day', 'class', ',', 'videos', 'organized', 'day-1', 'day-2', 'sections', '.', 'material', 'created', 'nick', 'parlante', 'working', 'engedu', 'group', 'google', '.', 'special', 'thanks', 'help', 'google', 'colleagues', 'john', 'cox', ',', 'steve', 'glassman', ',', 'piotr', 'kaminksi', ',', 'antoine', 'picard', '.', 'finally', 'thanks', 'google', 'director', 'maggie', 'johnson', 'enlightened', 'generosity', 'put', 'materials', 'internet', 'free', 'creative', 'commons', 'attribution', '2.5', 'license', '--', 'share', 'enjoy', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword = nltk.download('stopwords')\n",
    "\n",
    "tokens1_no_sw = [word for word in token1 if not word in stopwords.words()]\n",
    "tokens2_no_sw = [word for word in token2 if not word in stopwords.words()]\n",
    "tokens3_no_sw = [word for word in token3 if not word in stopwords.words()]\n",
    "print(tokens1_no_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'started', 'python', 'sections', 'linked', 'left', 'python', 'set', 'get', 'python', 'installed', 'machine', 'python', 'introduction', 'introduction', 'language', 'python', 'strings', 'starts', 'coding', 'material', 'leading', 'first', 'exercise', 'written', 'section', 'includes', 'link', 'code', 'exercise', 'section', 'material', 'lecture', 'videos', 'parallel', 'written', 'materials', 'introducing', 'python', 'strings', 'first', 'exercises', 'google', 'material', 'makes', 'intensive', 'class', 'videos', 'organized', 'sections', 'material', 'created', 'nick', 'parlante', 'working', 'engedu', 'group', 'google', 'special', 'thanks', 'help', 'google', 'colleagues', 'john', 'cox', 'steve', 'glassman', 'piotr', 'kaminksi', 'antoine', 'picard', 'finally', 'thanks', 'google', 'director', 'maggie', 'johnson', 'enlightened', 'generosity', 'put', 'materials', 'internet', 'free', 'creative', 'commons', 'attribution', 'license', 'share', 'enjoy']\n"
     ]
    }
   ],
   "source": [
    "words1 = [word for word in tokens1_no_sw if word.isalpha()]\n",
    "words2 = [word for word in tokens2_no_sw if word.isalpha()]\n",
    "words3 = [word for word in tokens3_no_sw if word.isalpha()]\n",
    "print(words1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) POS tagging for every term and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS1 = nltk.pos_tag(words1)\n",
    "POS2 = nltk.pos_tag(words2)\n",
    "POS3 = nltk.pos_tag(words3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('get', 'NN'), ('started', 'VBD'), ('python', 'JJ'), ('sections', 'NNS'), ('linked', 'VBN'), ('left', 'JJ'), ('python', 'NN'), ('set', 'VBN'), ('get', 'VB'), ('python', 'JJ'), ('installed', 'VBN'), ('machine', 'NN'), ('python', 'NN'), ('introduction', 'NN'), ('introduction', 'NN'), ('language', 'NN'), ('python', 'NN'), ('strings', 'NNS'), ('starts', 'VBZ'), ('coding', 'VBG'), ('material', 'NN'), ('leading', 'VBG'), ('first', 'JJ'), ('exercise', 'NN'), ('written', 'VBN'), ('section', 'NN'), ('includes', 'VBZ'), ('link', 'VBP'), ('code', 'NN'), ('exercise', 'NN'), ('section', 'NN'), ('material', 'NN'), ('lecture', 'NN'), ('videos', 'FW'), ('parallel', 'JJ'), ('written', 'VBN'), ('materials', 'NNS'), ('introducing', 'VBG'), ('python', 'NN'), ('strings', 'NNS'), ('first', 'JJ'), ('exercises', 'VBZ'), ('google', 'JJ'), ('material', 'NN'), ('makes', 'VBZ'), ('intensive', 'JJ'), ('class', 'NN'), ('videos', 'NN'), ('organized', 'VBN'), ('sections', 'NNS'), ('material', 'NN'), ('created', 'VBD'), ('nick', 'JJ'), ('parlante', 'NN'), ('working', 'VBG'), ('engedu', 'JJ'), ('group', 'NN'), ('google', 'VBD'), ('special', 'JJ'), ('thanks', 'NNS'), ('help', 'VBP'), ('google', 'VB'), ('colleagues', 'NNS'), ('john', 'VB'), ('cox', 'NNS'), ('steve', 'VBP'), ('glassman', 'NN'), ('piotr', 'NN'), ('kaminksi', 'FW'), ('antoine', 'JJ'), ('picard', 'NN'), ('finally', 'RB'), ('thanks', 'NNS'), ('google', 'VBP'), ('director', 'NN'), ('maggie', 'NN'), ('johnson', 'NN'), ('enlightened', 'VBD'), ('generosity', 'NN'), ('put', 'VBN'), ('materials', 'NNS'), ('internet', 'VBP'), ('free', 'JJ'), ('creative', 'JJ'), ('commons', 'NNS'), ('attribution', 'NN'), ('license', 'NN'), ('share', 'NN'), ('enjoy', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(POS1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('welcome', 'JJ'), ('google', 'NN'), ('python', 'NN'), ('online', 'VBP'), ('tutorial', 'NN'), ('based', 'VBN'), ('introductory', 'JJ'), ('python', 'JJ'), ('course', 'NN'), ('offered', 'VBN'), ('internally', 'RB'), ('originally', 'RB'), ('created', 'VBN'), ('python', 'NN'), ('days', 'NNS'), ('tried', 'VBD'), ('keep', 'JJ'), ('content', 'NN'), ('universal', 'NN'), ('exercises', 'VBZ'), ('relevant', 'JJ'), ('even', 'RB'), ('newer', 'JJR'), ('releases', 'NNS'), ('mentioned', 'VBD'), ('setup', 'JJ'), ('page', 'NN'), ('material', 'NN'), ('covers', 'VBZ'), ('python', 'JJ'), ('recommend', 'NN'), ('avoiding', 'VBG'), ('python', 'JJ'), ('recognize', 'VB'), ('future', 'JJ'), ('new', 'JJ'), ('features', 'NNS'), ('going', 'VBG'), ('good', 'JJ'), ('news', 'NN'), ('developers', 'NNS'), ('learning', 'VBG'), ('either', 'DT'), ('version', 'NN'), ('pick', 'NN'), ('without', 'IN'), ('much', 'JJ'), ('difficulty', 'NN'), ('know', 'VBP'), ('choosing', 'VBG'), ('python', 'NN'), ('check', 'NN'), ('post', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(POS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('need', 'NN'), ('quick', 'JJ'), ('learning', 'VBG'), ('python', 'NN'), ('first', 'JJ'), ('time', 'NN'), ('right', 'JJ'), ('place', 'NN'), ('let', 'VB'), ('get', 'VB'), ('started', 'VBN'), ('learning', 'VBG'), ('easiest', 'JJS'), ('coding', 'VBG'), ('languages', 'NNS'), ('right', 'RB'), ('need', 'VBP'), ('fret', 'RB'), ('coded', 'JJ'), ('time', 'NN'), ('finish', 'JJ'), ('course', 'NN'), ('pro', 'JJ'), ('python', 'NN'), ('python', 'NN'), ('great', 'JJ'), ('friendly', 'JJ'), ('language', 'NN'), ('use', 'NN'), ('learn', 'FW'), ('fun', 'NN'), ('adapted', 'VBD'), ('small', 'JJ'), ('large', 'JJ'), ('projects', 'NNS'), ('python', 'VBP'), ('cut', 'VB'), ('development', 'NN'), ('time', 'NN'), ('greatly', 'RB'), ('overall', 'JJ'), ('much', 'JJ'), ('faster', 'JJR'), ('write', 'JJ'), ('python', 'NN'), ('languages', 'NNS'), ('course', 'NN'), ('quick', 'JJ'), ('way', 'NN'), ('understand', 'IN'), ('major', 'JJ'), ('concepts', 'NNS'), ('python', 'VBP'), ('programming', 'VBG'), ('whiz', 'JJ'), ('time', 'NN'), ('course', 'NN'), ('everything', 'NN'), ('need', 'NN'), ('know', 'VBP'), ('get', 'VB'), ('started', 'VBN'), ('python', 'RB'), ('along', 'IN'), ('incentives', 'NNS'), ('begin', 'VBP'), ('basics', 'NNS'), ('python', 'VBP'), ('learning', 'VBG'), ('strings', 'NNS'), ('variables', 'NNS'), ('getting', 'VBG'), ('know', 'VBP'), ('data', 'NNS'), ('types', 'NNS'), ('soon', 'RB'), ('move', 'VBP'), ('loops', 'JJ'), ('conditions', 'NNS'), ('python', 'VBP'), ('afterwards', 'NNS'), ('discuss', 'JJ'), ('bit', 'NN'), ('file', 'JJ'), ('manipulation', 'NN'), ('functions', 'NNS'), ('know', 'VBP'), ('basics', 'NNS'), ('python', 'VBP'), ('hope', 'NN'), ('excited', 'VBN'), ('dive', 'JJ'), ('world', 'NN'), ('python', 'NN'), ('course', 'NN'), ('well', 'RB'), ('waiting', 'VBG'), ('let', 'VB'), ('get', 'VB'), ('started', 'VBN')]\n"
     ]
    }
   ],
   "source": [
    "print(POS3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature selection and tf-idf weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Using the result from question 1, do feature selection by only select the term with POS is Noun, Verb, and Adjective. Note that depending on which POS tagging method you use, you need to convert between POS tag and POS, (e.g. VB, VBZ, VBD are all VERBs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) After removing all terms that are not Verb, Noun, Adjective, compute tf-idf weighting for every term in the three documents for both cases: nonnormalization (tf * idf) and normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Display the result in the panda data frame as below (you have to display 2 results, one for non-normalization and one for normalization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Assume that the goal is to distinguish the differences between the three documents based on their features (term). Doing a feature selection based on POS in question 2 compared to not implementing a feature selection, do you think which option would be better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Based on the content of the 3 document files above, do you suggest a better feature selection method? (use POS but choose other POS tags or a different method or combination of methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
