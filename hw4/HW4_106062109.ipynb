{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4- POS and vectorize plain text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization & POS tagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Read 3 document (doc1.txt, doc2.txt, doc3.txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "file1 = read_data('./data/doc1.txt')\n",
    "file2 = read_data('./data/doc2.txt')\n",
    "file3 = read_data('./data/doc3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Normalize words: convert all upper-case letter to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to get started, the python sections are linked at the left -- python set up to get python installed on your machine, python introduction for an introduction to the language, and then python strings starts the coding material, leading to the first exercise. the end of each written section includes a link to the code exercise for that section's material. the lecture videos parallel the written materials, introducing python, then strings, then first exercises, and so on. at google, all this material makes up an intensive 2-day class, so the videos are organized as the day-1 and day-2 sections.\n",
      "\n",
      "this material was created by nick parlante working in the engedu group at google. special thanks for the help from my google colleagues john cox, steve glassman, piotr kaminksi, and antoine picard. and finally thanks to google and my director maggie johnson for the enlightened generosity to put these materials out on the internet for free under the creative commons attribution 2.5 license -- share and enjoy!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def normalized(file):\n",
    "    file = file.lower()\n",
    "    return file\n",
    "\n",
    "norm_file1 = normalized(file1)\n",
    "norm_file2 = normalized(file2)\n",
    "norm_file3 = normalized(file3)\n",
    "print(norm_file1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'get', 'started', ',', 'the', 'python', 'sections', 'are', 'linked', 'at', 'the', 'left', '--', 'python', 'set', 'up', 'to', 'get', 'python', 'installed', 'on', 'your', 'machine', ',', 'python', 'introduction', 'for', 'an', 'introduction', 'to', 'the', 'language', ',', 'and', 'then', 'python', 'strings', 'starts', 'the', 'coding', 'material', ',', 'leading', 'to', 'the', 'first', 'exercise', '.', 'the', 'end', 'of', 'each', 'written', 'section', 'includes', 'a', 'link', 'to', 'the', 'code', 'exercise', 'for', 'that', 'section', \"'s\", 'material', '.', 'the', 'lecture', 'videos', 'parallel', 'the', 'written', 'materials', ',', 'introducing', 'python', ',', 'then', 'strings', ',', 'then', 'first', 'exercises', ',', 'and', 'so', 'on', '.', 'at', 'google', ',', 'all', 'this', 'material', 'makes', 'up', 'an', 'intensive', '2-day', 'class', ',', 'so', 'the', 'videos', 'are', 'organized', 'as', 'the', 'day-1', 'and', 'day-2', 'sections', '.', 'this', 'material', 'was', 'created', 'by', 'nick', 'parlante', 'working', 'in', 'the', 'engedu', 'group', 'at', 'google', '.', 'special', 'thanks', 'for', 'the', 'help', 'from', 'my', 'google', 'colleagues', 'john', 'cox', ',', 'steve', 'glassman', ',', 'piotr', 'kaminksi', ',', 'and', 'antoine', 'picard', '.', 'and', 'finally', 'thanks', 'to', 'google', 'and', 'my', 'director', 'maggie', 'johnson', 'for', 'the', 'enlightened', 'generosity', 'to', 'put', 'these', 'materials', 'out', 'on', 'the', 'internet', 'for', 'free', 'under', 'the', 'creative', 'commons', 'attribution', '2.5', 'license', '--', 'share', 'and', 'enjoy', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "token1 = nltk.word_tokenize(norm_file1)\n",
    "token2 = nltk.word_tokenize(norm_file2)\n",
    "token3 = nltk.word_tokenize(norm_file3)\n",
    "# non normalized\n",
    "no_token1 = nltk.word_tokenize(file1)\n",
    "no_token2 = nltk.word_tokenize(file2)\n",
    "no_token3 = nltk.word_tokenize(file3)\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Remove stopwords using stopwords from nltk or other sources and remove punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'started', ',', 'python', 'sections', 'linked', 'left', '--', 'python', 'set', 'get', 'python', 'installed', 'machine', ',', 'python', 'introduction', 'introduction', 'language', ',', 'python', 'strings', 'starts', 'coding', 'material', ',', 'leading', 'first', 'exercise', '.', 'written', 'section', 'includes', 'link', 'code', 'exercise', 'section', \"'s\", 'material', '.', 'lecture', 'videos', 'parallel', 'written', 'materials', ',', 'introducing', 'python', ',', 'strings', ',', 'first', 'exercises', ',', '.', 'google', ',', 'material', 'makes', 'intensive', '2-day', 'class', ',', 'videos', 'organized', 'day-1', 'day-2', 'sections', '.', 'material', 'created', 'nick', 'parlante', 'working', 'engedu', 'group', 'google', '.', 'special', 'thanks', 'help', 'google', 'colleagues', 'john', 'cox', ',', 'steve', 'glassman', ',', 'piotr', 'kaminksi', ',', 'antoine', 'picard', '.', 'finally', 'thanks', 'google', 'director', 'maggie', 'johnson', 'enlightened', 'generosity', 'put', 'materials', 'internet', 'free', 'creative', 'commons', 'attribution', '2.5', 'license', '--', 'share', 'enjoy', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword = nltk.download('stopwords')\n",
    "\n",
    "tokens1_no_sw = [word for word in token1 if not word in stopwords.words()]\n",
    "tokens2_no_sw = [word for word in token2 if not word in stopwords.words()]\n",
    "tokens3_no_sw = [word for word in token3 if not word in stopwords.words()]\n",
    "# non normalized\n",
    "no_tokens1_no_sw = [word for word in no_token1 if not word in stopwords.words()]\n",
    "no_tokens2_no_sw = [word for word in no_token2 if not word in stopwords.words()]\n",
    "no_tokens3_no_sw = [word for word in no_token3 if not word in stopwords.words()]\n",
    "print(tokens1_no_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'started', 'python', 'sections', 'linked', 'left', 'python', 'set', 'get', 'python', 'installed', 'machine', 'python', 'introduction', 'introduction', 'language', 'python', 'strings', 'starts', 'coding', 'material', 'leading', 'first', 'exercise', 'written', 'section', 'includes', 'link', 'code', 'exercise', 'section', 'material', 'lecture', 'videos', 'parallel', 'written', 'materials', 'introducing', 'python', 'strings', 'first', 'exercises', 'google', 'material', 'makes', 'intensive', 'class', 'videos', 'organized', 'sections', 'material', 'created', 'nick', 'parlante', 'working', 'engedu', 'group', 'google', 'special', 'thanks', 'help', 'google', 'colleagues', 'john', 'cox', 'steve', 'glassman', 'piotr', 'kaminksi', 'antoine', 'picard', 'finally', 'thanks', 'google', 'director', 'maggie', 'johnson', 'enlightened', 'generosity', 'put', 'materials', 'internet', 'free', 'creative', 'commons', 'attribution', 'license', 'share', 'enjoy']\n"
     ]
    }
   ],
   "source": [
    "words1 = [word for word in tokens1_no_sw if word.isalpha()]\n",
    "words2 = [word for word in tokens2_no_sw if word.isalpha()]\n",
    "words3 = [word for word in tokens3_no_sw if word.isalpha()]\n",
    "# non normalized\n",
    "no_words1 = [word for word in no_tokens1_no_sw if word.isalpha()]\n",
    "no_words2 = [word for word in no_tokens2_no_sw if word.isalpha()]\n",
    "no_words3 = [word for word in no_tokens3_no_sw if word.isalpha()]\n",
    "print(words1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) POS tagging for every term and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('universal_tagset')\n",
    "pos1_tokens_uni = nltk.pos_tag(words1,tagset='universal')\n",
    "pos2_tokens_uni = nltk.pos_tag(words2,tagset='universal')\n",
    "pos3_tokens_uni = nltk.pos_tag(words3,tagset='universal')\n",
    "# non normalized\n",
    "no_pos1_tokens_uni = nltk.pos_tag(no_words1,tagset='universal')\n",
    "no_pos2_tokens_uni = nltk.pos_tag(no_words2,tagset='universal')\n",
    "no_pos3_tokens_uni = nltk.pos_tag(no_words3,tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('get', 'NOUN'), ('started', 'VERB'), ('python', 'ADJ'), ('sections', 'NOUN'), ('linked', 'VERB'), ('left', 'ADJ'), ('python', 'NOUN'), ('set', 'VERB'), ('get', 'VERB'), ('python', 'ADJ'), ('installed', 'VERB'), ('machine', 'NOUN'), ('python', 'NOUN'), ('introduction', 'NOUN'), ('introduction', 'NOUN'), ('language', 'NOUN'), ('python', 'NOUN'), ('strings', 'NOUN'), ('starts', 'VERB'), ('coding', 'VERB'), ('material', 'NOUN'), ('leading', 'VERB'), ('first', 'ADJ'), ('exercise', 'NOUN'), ('written', 'VERB'), ('section', 'NOUN'), ('includes', 'VERB'), ('link', 'VERB'), ('code', 'NOUN'), ('exercise', 'NOUN'), ('section', 'NOUN'), ('material', 'NOUN'), ('lecture', 'NOUN'), ('videos', 'X'), ('parallel', 'ADJ'), ('written', 'VERB'), ('materials', 'NOUN'), ('introducing', 'VERB'), ('python', 'NOUN'), ('strings', 'NOUN'), ('first', 'ADJ'), ('exercises', 'VERB'), ('google', 'ADJ'), ('material', 'NOUN'), ('makes', 'VERB'), ('intensive', 'ADJ'), ('class', 'NOUN'), ('videos', 'NOUN'), ('organized', 'VERB'), ('sections', 'NOUN'), ('material', 'NOUN'), ('created', 'VERB'), ('nick', 'ADJ'), ('parlante', 'NOUN'), ('working', 'VERB'), ('engedu', 'ADJ'), ('group', 'NOUN'), ('google', 'VERB'), ('special', 'ADJ'), ('thanks', 'NOUN'), ('help', 'VERB'), ('google', 'VERB'), ('colleagues', 'NOUN'), ('john', 'VERB'), ('cox', 'NOUN'), ('steve', 'VERB'), ('glassman', 'NOUN'), ('piotr', 'NOUN'), ('kaminksi', 'X'), ('antoine', 'ADJ'), ('picard', 'NOUN'), ('finally', 'ADV'), ('thanks', 'NOUN'), ('google', 'VERB'), ('director', 'NOUN'), ('maggie', 'NOUN'), ('johnson', 'NOUN'), ('enlightened', 'VERB'), ('generosity', 'NOUN'), ('put', 'VERB'), ('materials', 'NOUN'), ('internet', 'VERB'), ('free', 'ADJ'), ('creative', 'ADJ'), ('commons', 'NOUN'), ('attribution', 'NOUN'), ('license', 'NOUN'), ('share', 'NOUN'), ('enjoy', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "print(pos1_tokens_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('welcome', 'ADJ'), ('google', 'NOUN'), ('python', 'NOUN'), ('online', 'VERB'), ('tutorial', 'NOUN'), ('based', 'VERB'), ('introductory', 'ADJ'), ('python', 'ADJ'), ('course', 'NOUN'), ('offered', 'VERB'), ('internally', 'ADV'), ('originally', 'ADV'), ('created', 'VERB'), ('python', 'NOUN'), ('days', 'NOUN'), ('tried', 'VERB'), ('keep', 'ADJ'), ('content', 'NOUN'), ('universal', 'NOUN'), ('exercises', 'VERB'), ('relevant', 'ADJ'), ('even', 'ADV'), ('newer', 'ADJ'), ('releases', 'NOUN'), ('mentioned', 'VERB'), ('setup', 'ADJ'), ('page', 'NOUN'), ('material', 'NOUN'), ('covers', 'VERB'), ('python', 'ADJ'), ('recommend', 'NOUN'), ('avoiding', 'VERB'), ('python', 'ADJ'), ('recognize', 'VERB'), ('future', 'ADJ'), ('new', 'ADJ'), ('features', 'NOUN'), ('going', 'VERB'), ('good', 'ADJ'), ('news', 'NOUN'), ('developers', 'NOUN'), ('learning', 'VERB'), ('either', 'DET'), ('version', 'NOUN'), ('pick', 'NOUN'), ('without', 'ADP'), ('much', 'ADJ'), ('difficulty', 'NOUN'), ('know', 'VERB'), ('choosing', 'VERB'), ('python', 'NOUN'), ('check', 'NOUN'), ('post', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "print(pos2_tokens_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('need', 'NOUN'), ('quick', 'ADJ'), ('learning', 'VERB'), ('python', 'NOUN'), ('first', 'ADJ'), ('time', 'NOUN'), ('right', 'ADJ'), ('place', 'NOUN'), ('let', 'VERB'), ('get', 'VERB'), ('started', 'VERB'), ('learning', 'VERB'), ('easiest', 'ADJ'), ('coding', 'VERB'), ('languages', 'NOUN'), ('right', 'ADV'), ('need', 'VERB'), ('fret', 'ADV'), ('coded', 'ADJ'), ('time', 'NOUN'), ('finish', 'ADJ'), ('course', 'NOUN'), ('pro', 'ADJ'), ('python', 'NOUN'), ('python', 'NOUN'), ('great', 'ADJ'), ('friendly', 'ADJ'), ('language', 'NOUN'), ('use', 'NOUN'), ('learn', 'X'), ('fun', 'NOUN'), ('adapted', 'VERB'), ('small', 'ADJ'), ('large', 'ADJ'), ('projects', 'NOUN'), ('python', 'VERB'), ('cut', 'VERB'), ('development', 'NOUN'), ('time', 'NOUN'), ('greatly', 'ADV'), ('overall', 'ADJ'), ('much', 'ADJ'), ('faster', 'ADJ'), ('write', 'ADJ'), ('python', 'NOUN'), ('languages', 'NOUN'), ('course', 'NOUN'), ('quick', 'ADJ'), ('way', 'NOUN'), ('understand', 'ADP'), ('major', 'ADJ'), ('concepts', 'NOUN'), ('python', 'VERB'), ('programming', 'VERB'), ('whiz', 'ADJ'), ('time', 'NOUN'), ('course', 'NOUN'), ('everything', 'NOUN'), ('need', 'NOUN'), ('know', 'VERB'), ('get', 'VERB'), ('started', 'VERB'), ('python', 'ADV'), ('along', 'ADP'), ('incentives', 'NOUN'), ('begin', 'VERB'), ('basics', 'NOUN'), ('python', 'VERB'), ('learning', 'VERB'), ('strings', 'NOUN'), ('variables', 'NOUN'), ('getting', 'VERB'), ('know', 'VERB'), ('data', 'NOUN'), ('types', 'NOUN'), ('soon', 'ADV'), ('move', 'VERB'), ('loops', 'ADJ'), ('conditions', 'NOUN'), ('python', 'VERB'), ('afterwards', 'NOUN'), ('discuss', 'ADJ'), ('bit', 'NOUN'), ('file', 'ADJ'), ('manipulation', 'NOUN'), ('functions', 'NOUN'), ('know', 'VERB'), ('basics', 'NOUN'), ('python', 'VERB'), ('hope', 'NOUN'), ('excited', 'VERB'), ('dive', 'ADJ'), ('world', 'NOUN'), ('python', 'NOUN'), ('course', 'NOUN'), ('well', 'ADV'), ('waiting', 'VERB'), ('let', 'VERB'), ('get', 'VERB'), ('started', 'VERB')]\n"
     ]
    }
   ],
   "source": [
    "print(pos3_tokens_uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature selection and tf-idf weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Using the result from question 1, do feature selection by only select the term with POS is Noun, Verb, and Adjective. Note that depending on which POS tagging method you use, you need to convert between POS tag and POS, (e.g. VB, VBZ, VBD are all VERBs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_terms(tokens):\n",
    "    NVA_tokens =[] \n",
    "    for token in tokens:\n",
    "        if token[1] == \"NOUN\":\n",
    "            NVA_tokens += [token[0]]\n",
    "        elif token[1] == \"VERB\":\n",
    "            NVA_tokens += [token[0]]\n",
    "        elif token[1] == \"ADJ\":\n",
    "            NVA_tokens += [token[0]]\n",
    "    return NVA_tokens\n",
    "\n",
    "new_tokens1 = extract_terms(pos1_tokens_uni)\n",
    "new_tokens2 = extract_terms(pos2_tokens_uni)\n",
    "new_tokens3 = extract_terms(pos3_tokens_uni)\n",
    "\n",
    "no_new_tokens1 = extract_terms(no_pos1_tokens_uni)\n",
    "no_new_tokens2 = extract_terms(no_pos2_tokens_uni)\n",
    "no_new_tokens3 = extract_terms(no_pos3_tokens_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before extract: 86\n",
      "After extract: 89\n",
      "['get', 'started', 'python', 'sections', 'linked', 'left', 'python', 'set', 'get', 'python', 'installed', 'machine', 'python', 'introduction', 'introduction', 'language', 'python', 'strings', 'starts', 'coding', 'material', 'leading', 'first', 'exercise', 'written', 'section', 'includes', 'link', 'code', 'exercise', 'section', 'material', 'lecture', 'parallel', 'written', 'materials', 'introducing', 'python', 'strings', 'first', 'exercises', 'google', 'material', 'makes', 'intensive', 'class', 'videos', 'organized', 'sections', 'material', 'created', 'nick', 'parlante', 'working', 'engedu', 'group', 'google', 'special', 'thanks', 'help', 'google', 'colleagues', 'john', 'cox', 'steve', 'glassman', 'piotr', 'antoine', 'picard', 'thanks', 'google', 'director', 'maggie', 'johnson', 'enlightened', 'generosity', 'put', 'materials', 'internet', 'free', 'creative', 'commons', 'attribution', 'license', 'share', 'enjoy']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before extract:\",len(new_tokens1))\n",
    "print(\"After extract:\",len(pos1_tokens_uni))\n",
    "print(new_tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before extract: 48\n",
      "After extract: 53\n",
      "['welcome', 'google', 'python', 'online', 'tutorial', 'based', 'introductory', 'python', 'course', 'offered', 'created', 'python', 'days', 'tried', 'keep', 'content', 'universal', 'exercises', 'relevant', 'newer', 'releases', 'mentioned', 'setup', 'page', 'material', 'covers', 'python', 'recommend', 'avoiding', 'python', 'recognize', 'future', 'new', 'features', 'going', 'good', 'news', 'developers', 'learning', 'version', 'pick', 'much', 'difficulty', 'know', 'choosing', 'python', 'check', 'post']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before extract:\",len(new_tokens2))\n",
    "print(\"After extract:\",len(pos2_tokens_uni))\n",
    "print(new_tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before extract: 91\n",
      "After extract: 100\n",
      "['need', 'quick', 'learning', 'python', 'first', 'time', 'right', 'place', 'let', 'get', 'started', 'learning', 'easiest', 'coding', 'languages', 'need', 'coded', 'time', 'finish', 'course', 'pro', 'python', 'python', 'great', 'friendly', 'language', 'use', 'fun', 'adapted', 'small', 'large', 'projects', 'python', 'cut', 'development', 'time', 'overall', 'much', 'faster', 'write', 'python', 'languages', 'course', 'quick', 'way', 'major', 'concepts', 'python', 'programming', 'whiz', 'time', 'course', 'everything', 'need', 'know', 'get', 'started', 'incentives', 'begin', 'basics', 'python', 'learning', 'strings', 'variables', 'getting', 'know', 'data', 'types', 'move', 'loops', 'conditions', 'python', 'afterwards', 'discuss', 'bit', 'file', 'manipulation', 'functions', 'know', 'basics', 'python', 'hope', 'excited', 'dive', 'world', 'python', 'course', 'waiting', 'let', 'get', 'started']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before extract:\",len(new_tokens3))\n",
    "print(\"After extract:\",len(pos3_tokens_uni))\n",
    "print(new_tokens3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) After removing all terms that are not Verb, Noun, Adjective, compute tf-idf weighting for every term in the three documents for both cases: nonnormalization (tf * idf) and normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf function\n",
    "def tf(term, token_doc):\n",
    "    tf = token_doc.count(term) / len(token_doc)\n",
    "    return tf\n",
    "\n",
    "# create function to calculate how many doc contain the term \n",
    "def num_doc(word, token_doclist):\n",
    "    doc_count = 0\n",
    "    for doc_token in token_doclist:\n",
    "        if doc_token.count(word) > 0:\n",
    "            doc_count += 1\n",
    "    return doc_count\n",
    "\n",
    "import math \n",
    "# create function to calculate Inverse Document Frequency in doclist\n",
    "def idf(word, token_doclist):\n",
    "    df = num_doc(word, token_doclist)\n",
    "    return math.log10(len(token_doclist)/df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_all = {\"doc1\":new_tokens1, \"doc2\":new_tokens1, \"doc3\": new_tokens3}\n",
    "no_doc_all = {\"doc1\":no_new_tokens1, \"doc2\":no_new_tokens1, \"doc3\": no_new_tokens3}\n",
    "\n",
    "def Tf_idf(doc_all):\n",
    "    # create bag words\n",
    "    bag_words = []\n",
    "    for doc in doc_all.keys():\n",
    "        bag_words += doc_all[doc]\n",
    "    bag_words = set(bag_words) # without repeat words\n",
    "    #calculate idf for every word in bag_words\n",
    "    bag_words_idf = {} # declare \"bag_words_idf\" data structure is dictionary \n",
    "    for word in bag_words:\n",
    "        bag_words_idf[word] = idf(word, doc_all.values())\n",
    "\n",
    "    tf_idf = {} # store tfidf value\n",
    "    for doc in doc_all.keys():\n",
    "        tf_idf_doc = {} # as a dictionary to store tfidf of each doc\n",
    "        for term in set(doc_all[doc]):\n",
    "            tf_idf_doc[term] = tf(term, doc_all[doc]) * bag_words_idf[term]\n",
    "        tf_idf[doc] = tf_idf_doc\n",
    "        \n",
    "    return tf_idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Display the result in the panda data frame as below (you have to display 2 results, one for non-normalization and one for normalization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non- normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Afterwards</th>\n",
       "      <th>Antoine</th>\n",
       "      <th>Attribution</th>\n",
       "      <th>Commons</th>\n",
       "      <th>Cox</th>\n",
       "      <th>Creative</th>\n",
       "      <th>Glassman</th>\n",
       "      <th>Google</th>\n",
       "      <th>Introduction</th>\n",
       "      <th>John</th>\n",
       "      <th>Johnson</th>\n",
       "      <th>Kaminksi</th>\n",
       "      <th>Let</th>\n",
       "      <th>Maggie</th>\n",
       "      <th>Nick</th>\n",
       "      <th>Parlante</th>\n",
       "      <th>Picard</th>\n",
       "      <th>Piotr</th>\n",
       "      <th>Python</th>\n",
       "      <th>Set</th>\n",
       "      <th>Special</th>\n",
       "      <th>Steve</th>\n",
       "      <th>Strings</th>\n",
       "      <th>Up</th>\n",
       "      <th>Well</th>\n",
       "      <th>World</th>\n",
       "      <th>adapted</th>\n",
       "      <th>basics</th>\n",
       "      <th>begin</th>\n",
       "      <th>bit</th>\n",
       "      <th>class</th>\n",
       "      <th>code</th>\n",
       "      <th>coded</th>\n",
       "      <th>coding</th>\n",
       "      <th>colleagues</th>\n",
       "      <th>concepts</th>\n",
       "      <th>conditions</th>\n",
       "      <th>course</th>\n",
       "      <th>created</th>\n",
       "      <th>cut</th>\n",
       "      <th>data</th>\n",
       "      <th>development</th>\n",
       "      <th>director</th>\n",
       "      <th>discuss</th>\n",
       "      <th>dive</th>\n",
       "      <th>easiest</th>\n",
       "      <th>engEDU</th>\n",
       "      <th>enjoy</th>\n",
       "      <th>enlightened</th>\n",
       "      <th>everything</th>\n",
       "      <th>excited</th>\n",
       "      <th>exercise</th>\n",
       "      <th>exercises</th>\n",
       "      <th>faster</th>\n",
       "      <th>file</th>\n",
       "      <th>finish</th>\n",
       "      <th>first</th>\n",
       "      <th>free</th>\n",
       "      <th>fret</th>\n",
       "      <th>friendly</th>\n",
       "      <th>fun</th>\n",
       "      <th>functions</th>\n",
       "      <th>generosity</th>\n",
       "      <th>get</th>\n",
       "      <th>getting</th>\n",
       "      <th>great</th>\n",
       "      <th>group</th>\n",
       "      <th>help</th>\n",
       "      <th>hope</th>\n",
       "      <th>incentives</th>\n",
       "      <th>includes</th>\n",
       "      <th>installed</th>\n",
       "      <th>intensive</th>\n",
       "      <th>internet</th>\n",
       "      <th>introducing</th>\n",
       "      <th>introduction</th>\n",
       "      <th>know</th>\n",
       "      <th>language</th>\n",
       "      <th>languages</th>\n",
       "      <th>large</th>\n",
       "      <th>leading</th>\n",
       "      <th>learn</th>\n",
       "      <th>learning</th>\n",
       "      <th>lecture</th>\n",
       "      <th>left</th>\n",
       "      <th>license</th>\n",
       "      <th>link</th>\n",
       "      <th>linked</th>\n",
       "      <th>loops</th>\n",
       "      <th>machine</th>\n",
       "      <th>major</th>\n",
       "      <th>makes</th>\n",
       "      <th>manipulation</th>\n",
       "      <th>material</th>\n",
       "      <th>materials</th>\n",
       "      <th>move</th>\n",
       "      <th>much</th>\n",
       "      <th>need</th>\n",
       "      <th>organized</th>\n",
       "      <th>overall</th>\n",
       "      <th>parallel</th>\n",
       "      <th>place</th>\n",
       "      <th>pro</th>\n",
       "      <th>programming</th>\n",
       "      <th>projects</th>\n",
       "      <th>put</th>\n",
       "      <th>quick</th>\n",
       "      <th>right</th>\n",
       "      <th>section</th>\n",
       "      <th>sections</th>\n",
       "      <th>share</th>\n",
       "      <th>small</th>\n",
       "      <th>started</th>\n",
       "      <th>starts</th>\n",
       "      <th>strings</th>\n",
       "      <th>thanks</th>\n",
       "      <th>time</th>\n",
       "      <th>types</th>\n",
       "      <th>use</th>\n",
       "      <th>variables</th>\n",
       "      <th>videos</th>\n",
       "      <th>waiting</th>\n",
       "      <th>way</th>\n",
       "      <th>whiz</th>\n",
       "      <th>working</th>\n",
       "      <th>write</th>\n",
       "      <th>written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.008004</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008004</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.008004</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008004</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc3</th>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.010045</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.020089</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010045</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.015067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.015067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010045</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020089</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Afterwards   Antoine  Attribution   Commons       Cox  Creative  \\\n",
       "doc1         NaN  0.002001     0.002001  0.002001  0.002001  0.002001   \n",
       "doc2         NaN  0.002001     0.002001  0.002001  0.002001  0.002001   \n",
       "doc3    0.005022       NaN          NaN       NaN       NaN       NaN   \n",
       "\n",
       "      Glassman    Google  Introduction      John   Johnson  Kaminksi  \\\n",
       "doc1  0.002001  0.008004      0.002001  0.002001  0.002001  0.002001   \n",
       "doc2  0.002001  0.008004      0.002001  0.002001  0.002001  0.002001   \n",
       "doc3       NaN       NaN           NaN       NaN       NaN       NaN   \n",
       "\n",
       "           Let    Maggie      Nick  Parlante    Picard     Piotr  Python  \\\n",
       "doc1       NaN  0.002001  0.002001  0.002001  0.002001  0.002001     0.0   \n",
       "doc2       NaN  0.002001  0.002001  0.002001  0.002001  0.002001     0.0   \n",
       "doc3  0.010045       NaN       NaN       NaN       NaN       NaN     0.0   \n",
       "\n",
       "           Set   Special     Steve   Strings        Up      Well     World  \\\n",
       "doc1  0.002001  0.002001  0.002001  0.002001  0.002001       NaN       NaN   \n",
       "doc2  0.002001  0.002001  0.002001  0.002001  0.002001       NaN       NaN   \n",
       "doc3       NaN       NaN       NaN       NaN       NaN  0.005022  0.005022   \n",
       "\n",
       "       adapted    basics     begin       bit     class      code     coded  \\\n",
       "doc1       NaN       NaN       NaN       NaN  0.002001  0.002001       NaN   \n",
       "doc2       NaN       NaN       NaN       NaN  0.002001  0.002001       NaN   \n",
       "doc3  0.005022  0.010045  0.005022  0.005022       NaN       NaN  0.005022   \n",
       "\n",
       "      coding  colleagues  concepts  conditions    course   created       cut  \\\n",
       "doc1     0.0    0.002001       NaN         NaN       NaN  0.002001       NaN   \n",
       "doc2     0.0    0.002001       NaN         NaN       NaN  0.002001       NaN   \n",
       "doc3     0.0         NaN  0.005022    0.005022  0.020089       NaN  0.005022   \n",
       "\n",
       "          data  development  director   discuss      dive   easiest    engEDU  \\\n",
       "doc1       NaN          NaN  0.002001       NaN       NaN       NaN  0.002001   \n",
       "doc2       NaN          NaN  0.002001       NaN       NaN       NaN  0.002001   \n",
       "doc3  0.005022     0.005022       NaN  0.005022  0.005022  0.005022       NaN   \n",
       "\n",
       "         enjoy  enlightened  everything   excited  exercise  exercises  \\\n",
       "doc1  0.002001     0.002001         NaN       NaN  0.004002   0.002001   \n",
       "doc2  0.002001     0.002001         NaN       NaN  0.004002   0.002001   \n",
       "doc3       NaN          NaN    0.005022  0.005022       NaN        NaN   \n",
       "\n",
       "        faster      file    finish  first      free      fret  friendly  \\\n",
       "doc1       NaN       NaN       NaN    0.0  0.002001       NaN       NaN   \n",
       "doc2       NaN       NaN       NaN    0.0  0.002001       NaN       NaN   \n",
       "doc3  0.005022  0.005022  0.005022    0.0       NaN  0.005022  0.005022   \n",
       "\n",
       "           fun  functions  generosity  get   getting     great     group  \\\n",
       "doc1       NaN        NaN    0.002001  0.0       NaN       NaN  0.002001   \n",
       "doc2       NaN        NaN    0.002001  0.0       NaN       NaN  0.002001   \n",
       "doc3  0.005022   0.005022         NaN  0.0  0.005022  0.005022       NaN   \n",
       "\n",
       "          help      hope  incentives  includes  installed  intensive  \\\n",
       "doc1  0.002001       NaN         NaN  0.002001   0.002001   0.002001   \n",
       "doc2  0.002001       NaN         NaN  0.002001   0.002001   0.002001   \n",
       "doc3       NaN  0.005022    0.005022       NaN        NaN        NaN   \n",
       "\n",
       "      internet  introducing  introduction      know  language  languages  \\\n",
       "doc1  0.002001     0.002001      0.002001       NaN       0.0        NaN   \n",
       "doc2  0.002001     0.002001      0.002001       NaN       0.0        NaN   \n",
       "doc3       NaN          NaN           NaN  0.015067       0.0   0.010045   \n",
       "\n",
       "         large   leading     learn  learning   lecture      left   license  \\\n",
       "doc1       NaN  0.002001       NaN       NaN  0.002001  0.002001  0.002001   \n",
       "doc2       NaN  0.002001       NaN       NaN  0.002001  0.002001  0.002001   \n",
       "doc3  0.005022       NaN  0.005022  0.015067       NaN       NaN       NaN   \n",
       "\n",
       "          link    linked     loops   machine     major     makes  \\\n",
       "doc1  0.002001  0.002001       NaN  0.002001       NaN  0.002001   \n",
       "doc2  0.002001  0.002001       NaN  0.002001       NaN  0.002001   \n",
       "doc3       NaN       NaN  0.005022       NaN  0.005022       NaN   \n",
       "\n",
       "      manipulation  material  materials      move      much      need  \\\n",
       "doc1           NaN  0.008004   0.004002       NaN       NaN       NaN   \n",
       "doc2           NaN  0.008004   0.004002       NaN       NaN       NaN   \n",
       "doc3      0.005022       NaN        NaN  0.005022  0.005022  0.015067   \n",
       "\n",
       "      organized   overall  parallel     place       pro  programming  \\\n",
       "doc1   0.002001       NaN  0.002001       NaN       NaN          NaN   \n",
       "doc2   0.002001       NaN  0.002001       NaN       NaN          NaN   \n",
       "doc3        NaN  0.005022       NaN  0.005022  0.005022     0.005022   \n",
       "\n",
       "      projects       put     quick     right   section  sections     share  \\\n",
       "doc1       NaN  0.002001       NaN       NaN  0.004002  0.004002  0.002001   \n",
       "doc2       NaN  0.002001       NaN       NaN  0.004002  0.004002  0.002001   \n",
       "doc3  0.005022       NaN  0.010045  0.005022       NaN       NaN       NaN   \n",
       "\n",
       "         small  started    starts  strings    thanks      time     types  \\\n",
       "doc1       NaN      0.0  0.002001      0.0  0.004002       NaN       NaN   \n",
       "doc2       NaN      0.0  0.002001      0.0  0.004002       NaN       NaN   \n",
       "doc3  0.005022      0.0       NaN      0.0       NaN  0.020089  0.005022   \n",
       "\n",
       "           use  variables    videos   waiting       way      whiz   working  \\\n",
       "doc1       NaN        NaN  0.002001       NaN       NaN       NaN  0.002001   \n",
       "doc2       NaN        NaN  0.002001       NaN       NaN       NaN  0.002001   \n",
       "doc3  0.005022   0.005022       NaN  0.005022  0.005022  0.005022       NaN   \n",
       "\n",
       "         write   written  \n",
       "doc1       NaN  0.004002  \n",
       "doc2       NaN  0.004002  \n",
       "doc3  0.005022       NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "no_tf_idf = Tf_idf(no_doc_all)\n",
    "no_tf_dataframe = pd.DataFrame(no_tf_idf).transpose()\n",
    "no_tf_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adapted</th>\n",
       "      <th>afterwards</th>\n",
       "      <th>antoine</th>\n",
       "      <th>attribution</th>\n",
       "      <th>basics</th>\n",
       "      <th>begin</th>\n",
       "      <th>bit</th>\n",
       "      <th>class</th>\n",
       "      <th>code</th>\n",
       "      <th>coded</th>\n",
       "      <th>coding</th>\n",
       "      <th>colleagues</th>\n",
       "      <th>commons</th>\n",
       "      <th>concepts</th>\n",
       "      <th>conditions</th>\n",
       "      <th>course</th>\n",
       "      <th>cox</th>\n",
       "      <th>created</th>\n",
       "      <th>creative</th>\n",
       "      <th>cut</th>\n",
       "      <th>data</th>\n",
       "      <th>development</th>\n",
       "      <th>director</th>\n",
       "      <th>discuss</th>\n",
       "      <th>dive</th>\n",
       "      <th>easiest</th>\n",
       "      <th>engedu</th>\n",
       "      <th>enjoy</th>\n",
       "      <th>enlightened</th>\n",
       "      <th>everything</th>\n",
       "      <th>excited</th>\n",
       "      <th>exercise</th>\n",
       "      <th>exercises</th>\n",
       "      <th>faster</th>\n",
       "      <th>file</th>\n",
       "      <th>finish</th>\n",
       "      <th>first</th>\n",
       "      <th>free</th>\n",
       "      <th>friendly</th>\n",
       "      <th>fun</th>\n",
       "      <th>functions</th>\n",
       "      <th>generosity</th>\n",
       "      <th>get</th>\n",
       "      <th>getting</th>\n",
       "      <th>glassman</th>\n",
       "      <th>google</th>\n",
       "      <th>great</th>\n",
       "      <th>group</th>\n",
       "      <th>help</th>\n",
       "      <th>hope</th>\n",
       "      <th>incentives</th>\n",
       "      <th>includes</th>\n",
       "      <th>installed</th>\n",
       "      <th>intensive</th>\n",
       "      <th>internet</th>\n",
       "      <th>introducing</th>\n",
       "      <th>introduction</th>\n",
       "      <th>john</th>\n",
       "      <th>johnson</th>\n",
       "      <th>know</th>\n",
       "      <th>language</th>\n",
       "      <th>languages</th>\n",
       "      <th>large</th>\n",
       "      <th>leading</th>\n",
       "      <th>learning</th>\n",
       "      <th>lecture</th>\n",
       "      <th>left</th>\n",
       "      <th>let</th>\n",
       "      <th>license</th>\n",
       "      <th>link</th>\n",
       "      <th>linked</th>\n",
       "      <th>loops</th>\n",
       "      <th>machine</th>\n",
       "      <th>maggie</th>\n",
       "      <th>major</th>\n",
       "      <th>makes</th>\n",
       "      <th>manipulation</th>\n",
       "      <th>material</th>\n",
       "      <th>materials</th>\n",
       "      <th>move</th>\n",
       "      <th>much</th>\n",
       "      <th>need</th>\n",
       "      <th>nick</th>\n",
       "      <th>organized</th>\n",
       "      <th>overall</th>\n",
       "      <th>parallel</th>\n",
       "      <th>parlante</th>\n",
       "      <th>picard</th>\n",
       "      <th>piotr</th>\n",
       "      <th>place</th>\n",
       "      <th>pro</th>\n",
       "      <th>programming</th>\n",
       "      <th>projects</th>\n",
       "      <th>put</th>\n",
       "      <th>python</th>\n",
       "      <th>quick</th>\n",
       "      <th>right</th>\n",
       "      <th>section</th>\n",
       "      <th>sections</th>\n",
       "      <th>set</th>\n",
       "      <th>share</th>\n",
       "      <th>small</th>\n",
       "      <th>special</th>\n",
       "      <th>started</th>\n",
       "      <th>starts</th>\n",
       "      <th>steve</th>\n",
       "      <th>strings</th>\n",
       "      <th>thanks</th>\n",
       "      <th>time</th>\n",
       "      <th>types</th>\n",
       "      <th>use</th>\n",
       "      <th>variables</th>\n",
       "      <th>videos</th>\n",
       "      <th>waiting</th>\n",
       "      <th>way</th>\n",
       "      <th>whiz</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>write</th>\n",
       "      <th>written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.00819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00819</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.00819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00819</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004095</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc3</th>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010486</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.020972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010486</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.015729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.015729</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010486</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.020972</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       adapted  afterwards   antoine  attribution    basics     begin  \\\n",
       "doc1       NaN         NaN  0.002048     0.002048       NaN       NaN   \n",
       "doc2       NaN         NaN  0.002048     0.002048       NaN       NaN   \n",
       "doc3  0.005243    0.005243       NaN          NaN  0.010486  0.005243   \n",
       "\n",
       "           bit     class      code     coded  coding  colleagues   commons  \\\n",
       "doc1       NaN  0.002048  0.002048       NaN     0.0    0.002048  0.002048   \n",
       "doc2       NaN  0.002048  0.002048       NaN     0.0    0.002048  0.002048   \n",
       "doc3  0.005243       NaN       NaN  0.005243     0.0         NaN       NaN   \n",
       "\n",
       "      concepts  conditions    course       cox   created  creative       cut  \\\n",
       "doc1       NaN         NaN       NaN  0.002048  0.002048  0.002048       NaN   \n",
       "doc2       NaN         NaN       NaN  0.002048  0.002048  0.002048       NaN   \n",
       "doc3  0.005243    0.005243  0.020972       NaN       NaN       NaN  0.005243   \n",
       "\n",
       "          data  development  director   discuss      dive   easiest    engedu  \\\n",
       "doc1       NaN          NaN  0.002048       NaN       NaN       NaN  0.002048   \n",
       "doc2       NaN          NaN  0.002048       NaN       NaN       NaN  0.002048   \n",
       "doc3  0.005243     0.005243       NaN  0.005243  0.005243  0.005243       NaN   \n",
       "\n",
       "         enjoy  enlightened  everything   excited  exercise  exercises  \\\n",
       "doc1  0.002048     0.002048         NaN       NaN  0.004095   0.002048   \n",
       "doc2  0.002048     0.002048         NaN       NaN  0.004095   0.002048   \n",
       "doc3       NaN          NaN    0.005243  0.005243       NaN        NaN   \n",
       "\n",
       "        faster      file    finish  first      free  friendly       fun  \\\n",
       "doc1       NaN       NaN       NaN    0.0  0.002048       NaN       NaN   \n",
       "doc2       NaN       NaN       NaN    0.0  0.002048       NaN       NaN   \n",
       "doc3  0.005243  0.005243  0.005243    0.0       NaN  0.005243  0.005243   \n",
       "\n",
       "      functions  generosity  get   getting  glassman   google     great  \\\n",
       "doc1        NaN    0.002048  0.0       NaN  0.002048  0.00819       NaN   \n",
       "doc2        NaN    0.002048  0.0       NaN  0.002048  0.00819       NaN   \n",
       "doc3   0.005243         NaN  0.0  0.005243       NaN      NaN  0.005243   \n",
       "\n",
       "         group      help      hope  incentives  includes  installed  \\\n",
       "doc1  0.002048  0.002048       NaN         NaN  0.002048   0.002048   \n",
       "doc2  0.002048  0.002048       NaN         NaN  0.002048   0.002048   \n",
       "doc3       NaN       NaN  0.005243    0.005243       NaN        NaN   \n",
       "\n",
       "      intensive  internet  introducing  introduction      john   johnson  \\\n",
       "doc1   0.002048  0.002048     0.002048      0.004095  0.002048  0.002048   \n",
       "doc2   0.002048  0.002048     0.002048      0.004095  0.002048  0.002048   \n",
       "doc3        NaN       NaN          NaN           NaN       NaN       NaN   \n",
       "\n",
       "          know  language  languages     large   leading  learning   lecture  \\\n",
       "doc1       NaN       0.0        NaN       NaN  0.002048       NaN  0.002048   \n",
       "doc2       NaN       0.0        NaN       NaN  0.002048       NaN  0.002048   \n",
       "doc3  0.015729       0.0   0.010486  0.005243       NaN  0.015729       NaN   \n",
       "\n",
       "          left       let   license      link    linked     loops   machine  \\\n",
       "doc1  0.002048       NaN  0.002048  0.002048  0.002048       NaN  0.002048   \n",
       "doc2  0.002048       NaN  0.002048  0.002048  0.002048       NaN  0.002048   \n",
       "doc3       NaN  0.010486       NaN       NaN       NaN  0.005243       NaN   \n",
       "\n",
       "        maggie     major     makes  manipulation  material  materials  \\\n",
       "doc1  0.002048       NaN  0.002048           NaN   0.00819   0.004095   \n",
       "doc2  0.002048       NaN  0.002048           NaN   0.00819   0.004095   \n",
       "doc3       NaN  0.005243       NaN      0.005243       NaN        NaN   \n",
       "\n",
       "          move      much      need      nick  organized   overall  parallel  \\\n",
       "doc1       NaN       NaN       NaN  0.002048   0.002048       NaN  0.002048   \n",
       "doc2       NaN       NaN       NaN  0.002048   0.002048       NaN  0.002048   \n",
       "doc3  0.005243  0.005243  0.015729       NaN        NaN  0.005243       NaN   \n",
       "\n",
       "      parlante    picard     piotr     place       pro  programming  projects  \\\n",
       "doc1  0.002048  0.002048  0.002048       NaN       NaN          NaN       NaN   \n",
       "doc2  0.002048  0.002048  0.002048       NaN       NaN          NaN       NaN   \n",
       "doc3       NaN       NaN       NaN  0.005243  0.005243     0.005243  0.005243   \n",
       "\n",
       "           put  python     quick     right   section  sections       set  \\\n",
       "doc1  0.002048     0.0       NaN       NaN  0.004095  0.004095  0.002048   \n",
       "doc2  0.002048     0.0       NaN       NaN  0.004095  0.004095  0.002048   \n",
       "doc3       NaN     0.0  0.010486  0.005243       NaN       NaN       NaN   \n",
       "\n",
       "         share     small   special  started    starts     steve  strings  \\\n",
       "doc1  0.002048       NaN  0.002048      0.0  0.002048  0.002048      0.0   \n",
       "doc2  0.002048       NaN  0.002048      0.0  0.002048  0.002048      0.0   \n",
       "doc3       NaN  0.005243       NaN      0.0       NaN       NaN      0.0   \n",
       "\n",
       "        thanks      time     types       use  variables    videos   waiting  \\\n",
       "doc1  0.004095       NaN       NaN       NaN        NaN  0.002048       NaN   \n",
       "doc2  0.004095       NaN       NaN       NaN        NaN  0.002048       NaN   \n",
       "doc3       NaN  0.020972  0.005243  0.005243   0.005243       NaN  0.005243   \n",
       "\n",
       "           way      whiz   working     world     write   written  \n",
       "doc1       NaN       NaN  0.002048       NaN       NaN  0.004095  \n",
       "doc2       NaN       NaN  0.002048       NaN       NaN  0.004095  \n",
       "doc3  0.005243  0.005243       NaN  0.005243  0.005243       NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = Tf_idf(doc_all)\n",
    "tf_dataframe = pd.DataFrame(tf_idf).transpose()\n",
    "tf_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Assume that the goal is to distinguish the differences between the three documents based on their features (term). Doing a feature selection based on POS in question 2 compared to not implementing a feature selection, do you think which option would be better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that doing a feature selection based on POS is better than without feature selection.\n",
    "Since if we can delete those non-important terms and get more focus on those meaningful terms,\n",
    "it indeed saves our time.\n",
    "By pick those useful and unique terms out, we can clearly see where are the differencies and how they distributed.\n",
    "Also, use POS method, we can choose which terms we want to focus on this time and modify the filter in different situation.\n",
    "Therefore, I think we should do feature selection with POS method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Feature Selection?\n",
    "\n",
    "1. Curse of dimensionality  Overfitting:\n",
    "If we have more columns in the data than the number of rows, we will be able to fit our training data perfectly, but that wont generalize to the new samples. And thus we learn absolutely nothing.\n",
    "2. Occams Razor:\n",
    "We want our models to be simple and explainable. We lose explainability when we have a lot of features.\n",
    "3. Garbage In Garbage out:\n",
    "Most of the times, we will have many non-informative features. For Example, Name or ID variables. Poor-quality input will produce Poor-Quality output.\n",
    "Also, a large number of features make a model bulky, time-taking, and harder to implement in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Based on the content of the 3 document files above, do you suggest a better feature selection method? (use POS but choose other POS tags or a different method or combination of methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think using POS by choosing noun, verb and adjective are enough since they are the most frequently used terms.\n",
    "Also, since the words in 3 documents are quite similar, so there's no meaning to choose other more tags.\n",
    "If we want to imporve the performance, I think we should use other methods. \n",
    "Below is the method I recommend and I think that the result will be much better than only doing POS feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two-stage feature selection method\n",
    "The proposed method is a combination of FCD and LSI. \n",
    "Firstly, we select features by the FCD feature selection method to reduce the feature numbers observably. \n",
    "Secondly, we apply LSI to construct a new conceptual vector space. \n",
    "The two-stage feature selection method conjugates the vector space model and the semantic feature space model. \n",
    "The related strategy for the proposed method is stated in detail as follows:\n",
    "\n",
    "Step 1. Remove stop words, punctuation, and non-alphanumeric text.\n",
    "\n",
    "Step 2. Calculate the normalized TFIDF in the corresponding element of the weight matrix.\n",
    "\n",
    "Step 3. Select the features according to the FCD method and get a new vector space model.\n",
    "\n",
    "Step 4. Construct the new semantic space model by means of LSI.\n",
    "\n",
    "Step 5. Use the SVM classifier on the semantic space model.\n",
    "\n",
    "Step 6. Obtain the categorization performance over the data set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
