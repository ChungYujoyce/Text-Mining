{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4- POS and vectorize plain text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization & POS tagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Read 3 document (doc1.txt, doc2.txt, doc3.txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "file1 = read_data('./data/doc1.txt')\n",
    "file2 = read_data('./data/doc2.txt')\n",
    "file3 = read_data('./data/doc3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Normalize words: convert all upper-case letter to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to get started, the python sections are linked at the left -- python set up to get python installed on your machine, python introduction for an introduction to the language, and then python strings starts the coding material, leading to the first exercise. the end of each written section includes a link to the code exercise for that section's material. the lecture videos parallel the written materials, introducing python, then strings, then first exercises, and so on. at google, all this material makes up an intensive 2-day class, so the videos are organized as the day-1 and day-2 sections.\n",
      "\n",
      "this material was created by nick parlante working in the engedu group at google. special thanks for the help from my google colleagues john cox, steve glassman, piotr kaminksi, and antoine picard. and finally thanks to google and my director maggie johnson for the enlightened generosity to put these materials out on the internet for free under the creative commons attribution 2.5 license -- share and enjoy!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def normalized(file):\n",
    "    file = file.lower()\n",
    "    return file\n",
    "\n",
    "norm_file1 = normalized(file1)\n",
    "norm_file2 = normalized(file2)\n",
    "norm_file3 = normalized(file3)\n",
    "print(norm_file1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'get', 'started', ',', 'the', 'python', 'sections', 'are', 'linked', 'at', 'the', 'left', '--', 'python', 'set', 'up', 'to', 'get', 'python', 'installed', 'on', 'your', 'machine', ',', 'python', 'introduction', 'for', 'an', 'introduction', 'to', 'the', 'language', ',', 'and', 'then', 'python', 'strings', 'starts', 'the', 'coding', 'material', ',', 'leading', 'to', 'the', 'first', 'exercise', '.', 'the', 'end', 'of', 'each', 'written', 'section', 'includes', 'a', 'link', 'to', 'the', 'code', 'exercise', 'for', 'that', 'section', \"'s\", 'material', '.', 'the', 'lecture', 'videos', 'parallel', 'the', 'written', 'materials', ',', 'introducing', 'python', ',', 'then', 'strings', ',', 'then', 'first', 'exercises', ',', 'and', 'so', 'on', '.', 'at', 'google', ',', 'all', 'this', 'material', 'makes', 'up', 'an', 'intensive', '2-day', 'class', ',', 'so', 'the', 'videos', 'are', 'organized', 'as', 'the', 'day-1', 'and', 'day-2', 'sections', '.', 'this', 'material', 'was', 'created', 'by', 'nick', 'parlante', 'working', 'in', 'the', 'engedu', 'group', 'at', 'google', '.', 'special', 'thanks', 'for', 'the', 'help', 'from', 'my', 'google', 'colleagues', 'john', 'cox', ',', 'steve', 'glassman', ',', 'piotr', 'kaminksi', ',', 'and', 'antoine', 'picard', '.', 'and', 'finally', 'thanks', 'to', 'google', 'and', 'my', 'director', 'maggie', 'johnson', 'for', 'the', 'enlightened', 'generosity', 'to', 'put', 'these', 'materials', 'out', 'on', 'the', 'internet', 'for', 'free', 'under', 'the', 'creative', 'commons', 'attribution', '2.5', 'license', '--', 'share', 'and', 'enjoy', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "token1 = nltk.word_tokenize(norm_file1)\n",
    "token2 = nltk.word_tokenize(norm_file2)\n",
    "token3 = nltk.word_tokenize(norm_file3)\n",
    "# non normalized\n",
    "no_token1 = nltk.word_tokenize(file1)\n",
    "no_token2 = nltk.word_tokenize(file2)\n",
    "no_token3 = nltk.word_tokenize(file3)\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Remove stopwords using stopwords from nltk or other sources and remove punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'started', ',', 'python', 'sections', 'linked', 'left', '--', 'python', 'set', 'get', 'python', 'installed', 'machine', ',', 'python', 'introduction', 'introduction', 'language', ',', 'python', 'strings', 'starts', 'coding', 'material', ',', 'leading', 'first', 'exercise', '.', 'written', 'section', 'includes', 'link', 'code', 'exercise', 'section', \"'s\", 'material', '.', 'lecture', 'videos', 'parallel', 'written', 'materials', ',', 'introducing', 'python', ',', 'strings', ',', 'first', 'exercises', ',', '.', 'google', ',', 'material', 'makes', 'intensive', '2-day', 'class', ',', 'videos', 'organized', 'day-1', 'day-2', 'sections', '.', 'material', 'created', 'nick', 'parlante', 'working', 'engedu', 'group', 'google', '.', 'special', 'thanks', 'help', 'google', 'colleagues', 'john', 'cox', ',', 'steve', 'glassman', ',', 'piotr', 'kaminksi', ',', 'antoine', 'picard', '.', 'finally', 'thanks', 'google', 'director', 'maggie', 'johnson', 'enlightened', 'generosity', 'put', 'materials', 'internet', 'free', 'creative', 'commons', 'attribution', '2.5', 'license', '--', 'share', 'enjoy', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword = nltk.download('stopwords')\n",
    "\n",
    "tokens1_no_sw = [word for word in token1 if not word in stopwords.words()]\n",
    "tokens2_no_sw = [word for word in token2 if not word in stopwords.words()]\n",
    "tokens3_no_sw = [word for word in token3 if not word in stopwords.words()]\n",
    "# non normalized\n",
    "no_tokens1_no_sw = [word for word in no_token1 if not word in stopwords.words()]\n",
    "no_tokens2_no_sw = [word for word in no_token2 if not word in stopwords.words()]\n",
    "no_tokens3_no_sw = [word for word in no_token3 if not word in stopwords.words()]\n",
    "print(tokens1_no_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'started', 'python', 'sections', 'linked', 'left', 'python', 'set', 'get', 'python', 'installed', 'machine', 'python', 'introduction', 'introduction', 'language', 'python', 'strings', 'starts', 'coding', 'material', 'leading', 'first', 'exercise', 'written', 'section', 'includes', 'link', 'code', 'exercise', 'section', 'material', 'lecture', 'videos', 'parallel', 'written', 'materials', 'introducing', 'python', 'strings', 'first', 'exercises', 'google', 'material', 'makes', 'intensive', 'class', 'videos', 'organized', 'sections', 'material', 'created', 'nick', 'parlante', 'working', 'engedu', 'group', 'google', 'special', 'thanks', 'help', 'google', 'colleagues', 'john', 'cox', 'steve', 'glassman', 'piotr', 'kaminksi', 'antoine', 'picard', 'finally', 'thanks', 'google', 'director', 'maggie', 'johnson', 'enlightened', 'generosity', 'put', 'materials', 'internet', 'free', 'creative', 'commons', 'attribution', 'license', 'share', 'enjoy']\n"
     ]
    }
   ],
   "source": [
    "words1 = [word for word in tokens1_no_sw if word.isalpha()]\n",
    "words2 = [word for word in tokens2_no_sw if word.isalpha()]\n",
    "words3 = [word for word in tokens3_no_sw if word.isalpha()]\n",
    "# non normalized\n",
    "no_words1 = [word for word in no_tokens1_no_sw if word.isalpha()]\n",
    "no_words2 = [word for word in no_tokens2_no_sw if word.isalpha()]\n",
    "no_words3 = [word for word in no_tokens3_no_sw if word.isalpha()]\n",
    "print(words1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) POS tagging for every term and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('universal_tagset')\n",
    "pos1_tokens_uni = nltk.pos_tag(words1,tagset='universal')\n",
    "pos2_tokens_uni = nltk.pos_tag(words2,tagset='universal')\n",
    "pos3_tokens_uni = nltk.pos_tag(words3,tagset='universal')\n",
    "# non normalized\n",
    "no_pos1_tokens_uni = nltk.pos_tag(no_words1,tagset='universal')\n",
    "no_pos2_tokens_uni = nltk.pos_tag(no_words2,tagset='universal')\n",
    "no_pos3_tokens_uni = nltk.pos_tag(no_words3,tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('get', 'NOUN'), ('started', 'VERB'), ('python', 'ADJ'), ('sections', 'NOUN'), ('linked', 'VERB'), ('left', 'ADJ'), ('python', 'NOUN'), ('set', 'VERB'), ('get', 'VERB'), ('python', 'ADJ'), ('installed', 'VERB'), ('machine', 'NOUN'), ('python', 'NOUN'), ('introduction', 'NOUN'), ('introduction', 'NOUN'), ('language', 'NOUN'), ('python', 'NOUN'), ('strings', 'NOUN'), ('starts', 'VERB'), ('coding', 'VERB'), ('material', 'NOUN'), ('leading', 'VERB'), ('first', 'ADJ'), ('exercise', 'NOUN'), ('written', 'VERB'), ('section', 'NOUN'), ('includes', 'VERB'), ('link', 'VERB'), ('code', 'NOUN'), ('exercise', 'NOUN'), ('section', 'NOUN'), ('material', 'NOUN'), ('lecture', 'NOUN'), ('videos', 'X'), ('parallel', 'ADJ'), ('written', 'VERB'), ('materials', 'NOUN'), ('introducing', 'VERB'), ('python', 'NOUN'), ('strings', 'NOUN'), ('first', 'ADJ'), ('exercises', 'VERB'), ('google', 'ADJ'), ('material', 'NOUN'), ('makes', 'VERB'), ('intensive', 'ADJ'), ('class', 'NOUN'), ('videos', 'NOUN'), ('organized', 'VERB'), ('sections', 'NOUN'), ('material', 'NOUN'), ('created', 'VERB'), ('nick', 'ADJ'), ('parlante', 'NOUN'), ('working', 'VERB'), ('engedu', 'ADJ'), ('group', 'NOUN'), ('google', 'VERB'), ('special', 'ADJ'), ('thanks', 'NOUN'), ('help', 'VERB'), ('google', 'VERB'), ('colleagues', 'NOUN'), ('john', 'VERB'), ('cox', 'NOUN'), ('steve', 'VERB'), ('glassman', 'NOUN'), ('piotr', 'NOUN'), ('kaminksi', 'X'), ('antoine', 'ADJ'), ('picard', 'NOUN'), ('finally', 'ADV'), ('thanks', 'NOUN'), ('google', 'VERB'), ('director', 'NOUN'), ('maggie', 'NOUN'), ('johnson', 'NOUN'), ('enlightened', 'VERB'), ('generosity', 'NOUN'), ('put', 'VERB'), ('materials', 'NOUN'), ('internet', 'VERB'), ('free', 'ADJ'), ('creative', 'ADJ'), ('commons', 'NOUN'), ('attribution', 'NOUN'), ('license', 'NOUN'), ('share', 'NOUN'), ('enjoy', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "print(pos1_tokens_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('welcome', 'ADJ'), ('google', 'NOUN'), ('python', 'NOUN'), ('online', 'VERB'), ('tutorial', 'NOUN'), ('based', 'VERB'), ('introductory', 'ADJ'), ('python', 'ADJ'), ('course', 'NOUN'), ('offered', 'VERB'), ('internally', 'ADV'), ('originally', 'ADV'), ('created', 'VERB'), ('python', 'NOUN'), ('days', 'NOUN'), ('tried', 'VERB'), ('keep', 'ADJ'), ('content', 'NOUN'), ('universal', 'NOUN'), ('exercises', 'VERB'), ('relevant', 'ADJ'), ('even', 'ADV'), ('newer', 'ADJ'), ('releases', 'NOUN'), ('mentioned', 'VERB'), ('setup', 'ADJ'), ('page', 'NOUN'), ('material', 'NOUN'), ('covers', 'VERB'), ('python', 'ADJ'), ('recommend', 'NOUN'), ('avoiding', 'VERB'), ('python', 'ADJ'), ('recognize', 'VERB'), ('future', 'ADJ'), ('new', 'ADJ'), ('features', 'NOUN'), ('going', 'VERB'), ('good', 'ADJ'), ('news', 'NOUN'), ('developers', 'NOUN'), ('learning', 'VERB'), ('either', 'DET'), ('version', 'NOUN'), ('pick', 'NOUN'), ('without', 'ADP'), ('much', 'ADJ'), ('difficulty', 'NOUN'), ('know', 'VERB'), ('choosing', 'VERB'), ('python', 'NOUN'), ('check', 'NOUN'), ('post', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "print(pos2_tokens_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('need', 'NOUN'), ('quick', 'ADJ'), ('learning', 'VERB'), ('python', 'NOUN'), ('first', 'ADJ'), ('time', 'NOUN'), ('right', 'ADJ'), ('place', 'NOUN'), ('let', 'VERB'), ('get', 'VERB'), ('started', 'VERB'), ('learning', 'VERB'), ('easiest', 'ADJ'), ('coding', 'VERB'), ('languages', 'NOUN'), ('right', 'ADV'), ('need', 'VERB'), ('fret', 'ADV'), ('coded', 'ADJ'), ('time', 'NOUN'), ('finish', 'ADJ'), ('course', 'NOUN'), ('pro', 'ADJ'), ('python', 'NOUN'), ('python', 'NOUN'), ('great', 'ADJ'), ('friendly', 'ADJ'), ('language', 'NOUN'), ('use', 'NOUN'), ('learn', 'X'), ('fun', 'NOUN'), ('adapted', 'VERB'), ('small', 'ADJ'), ('large', 'ADJ'), ('projects', 'NOUN'), ('python', 'VERB'), ('cut', 'VERB'), ('development', 'NOUN'), ('time', 'NOUN'), ('greatly', 'ADV'), ('overall', 'ADJ'), ('much', 'ADJ'), ('faster', 'ADJ'), ('write', 'ADJ'), ('python', 'NOUN'), ('languages', 'NOUN'), ('course', 'NOUN'), ('quick', 'ADJ'), ('way', 'NOUN'), ('understand', 'ADP'), ('major', 'ADJ'), ('concepts', 'NOUN'), ('python', 'VERB'), ('programming', 'VERB'), ('whiz', 'ADJ'), ('time', 'NOUN'), ('course', 'NOUN'), ('everything', 'NOUN'), ('need', 'NOUN'), ('know', 'VERB'), ('get', 'VERB'), ('started', 'VERB'), ('python', 'ADV'), ('along', 'ADP'), ('incentives', 'NOUN'), ('begin', 'VERB'), ('basics', 'NOUN'), ('python', 'VERB'), ('learning', 'VERB'), ('strings', 'NOUN'), ('variables', 'NOUN'), ('getting', 'VERB'), ('know', 'VERB'), ('data', 'NOUN'), ('types', 'NOUN'), ('soon', 'ADV'), ('move', 'VERB'), ('loops', 'ADJ'), ('conditions', 'NOUN'), ('python', 'VERB'), ('afterwards', 'NOUN'), ('discuss', 'ADJ'), ('bit', 'NOUN'), ('file', 'ADJ'), ('manipulation', 'NOUN'), ('functions', 'NOUN'), ('know', 'VERB'), ('basics', 'NOUN'), ('python', 'VERB'), ('hope', 'NOUN'), ('excited', 'VERB'), ('dive', 'ADJ'), ('world', 'NOUN'), ('python', 'NOUN'), ('course', 'NOUN'), ('well', 'ADV'), ('waiting', 'VERB'), ('let', 'VERB'), ('get', 'VERB'), ('started', 'VERB')]\n"
     ]
    }
   ],
   "source": [
    "print(pos3_tokens_uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature selection and tf-idf weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Using the result from question 1, do feature selection by only select the term with POS is Noun, Verb, and Adjective. Note that depending on which POS tagging method you use, you need to convert between POS tag and POS, (e.g. VB, VBZ, VBD are all VERBs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_terms(tokens):\n",
    "    NVA_tokens =[] \n",
    "    for token in tokens:\n",
    "        if token[1] == \"NOUN\":\n",
    "            NVA_tokens += [token[0]]\n",
    "        elif token[1] == \"VERB\":\n",
    "            NVA_tokens += [token[0]]\n",
    "        elif token[1] == \"ADJ\":\n",
    "            NVA_tokens += [token[0]]\n",
    "    return NVA_tokens\n",
    "\n",
    "new_tokens1 = extract_terms(pos1_tokens_uni)\n",
    "new_tokens2 = extract_terms(pos2_tokens_uni)\n",
    "new_tokens3 = extract_terms(pos3_tokens_uni)\n",
    "\n",
    "no_new_tokens1 = extract_terms(no_pos1_tokens_uni)\n",
    "no_new_tokens2 = extract_terms(no_pos2_tokens_uni)\n",
    "no_new_tokens3 = extract_terms(no_pos3_tokens_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before extract: 86\n",
      "After extract: 89\n",
      "['get', 'started', 'python', 'sections', 'linked', 'left', 'python', 'set', 'get', 'python', 'installed', 'machine', 'python', 'introduction', 'introduction', 'language', 'python', 'strings', 'starts', 'coding', 'material', 'leading', 'first', 'exercise', 'written', 'section', 'includes', 'link', 'code', 'exercise', 'section', 'material', 'lecture', 'parallel', 'written', 'materials', 'introducing', 'python', 'strings', 'first', 'exercises', 'google', 'material', 'makes', 'intensive', 'class', 'videos', 'organized', 'sections', 'material', 'created', 'nick', 'parlante', 'working', 'engedu', 'group', 'google', 'special', 'thanks', 'help', 'google', 'colleagues', 'john', 'cox', 'steve', 'glassman', 'piotr', 'antoine', 'picard', 'thanks', 'google', 'director', 'maggie', 'johnson', 'enlightened', 'generosity', 'put', 'materials', 'internet', 'free', 'creative', 'commons', 'attribution', 'license', 'share', 'enjoy']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before extract:\",len(new_tokens1))\n",
    "print(\"After extract:\",len(pos1_tokens_uni))\n",
    "print(new_tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before extract: 48\n",
      "After extract: 53\n",
      "['welcome', 'google', 'python', 'online', 'tutorial', 'based', 'introductory', 'python', 'course', 'offered', 'created', 'python', 'days', 'tried', 'keep', 'content', 'universal', 'exercises', 'relevant', 'newer', 'releases', 'mentioned', 'setup', 'page', 'material', 'covers', 'python', 'recommend', 'avoiding', 'python', 'recognize', 'future', 'new', 'features', 'going', 'good', 'news', 'developers', 'learning', 'version', 'pick', 'much', 'difficulty', 'know', 'choosing', 'python', 'check', 'post']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before extract:\",len(new_tokens2))\n",
    "print(\"After extract:\",len(pos2_tokens_uni))\n",
    "print(new_tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before extract: 91\n",
      "After extract: 100\n",
      "['need', 'quick', 'learning', 'python', 'first', 'time', 'right', 'place', 'let', 'get', 'started', 'learning', 'easiest', 'coding', 'languages', 'need', 'coded', 'time', 'finish', 'course', 'pro', 'python', 'python', 'great', 'friendly', 'language', 'use', 'fun', 'adapted', 'small', 'large', 'projects', 'python', 'cut', 'development', 'time', 'overall', 'much', 'faster', 'write', 'python', 'languages', 'course', 'quick', 'way', 'major', 'concepts', 'python', 'programming', 'whiz', 'time', 'course', 'everything', 'need', 'know', 'get', 'started', 'incentives', 'begin', 'basics', 'python', 'learning', 'strings', 'variables', 'getting', 'know', 'data', 'types', 'move', 'loops', 'conditions', 'python', 'afterwards', 'discuss', 'bit', 'file', 'manipulation', 'functions', 'know', 'basics', 'python', 'hope', 'excited', 'dive', 'world', 'python', 'course', 'waiting', 'let', 'get', 'started']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before extract:\",len(new_tokens3))\n",
    "print(\"After extract:\",len(pos3_tokens_uni))\n",
    "print(new_tokens3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) After removing all terms that are not Verb, Noun, Adjective, compute tf-idf weighting for every term in the three documents for both cases: nonnormalization (tf * idf) and normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf function\n",
    "def tf(term, token_doc):\n",
    "    tf = token_doc.count(term) / len(token_doc)\n",
    "    return tf\n",
    "\n",
    "# create function to calculate how many doc contain the term \n",
    "def num_doc(word, token_doclist):\n",
    "    doc_count = 0\n",
    "    for doc_token in token_doclist:\n",
    "        if doc_token.count(word) > 0:\n",
    "            doc_count += 1\n",
    "    return doc_count\n",
    "\n",
    "import math \n",
    "# create function to calculate Inverse Document Frequency in doclist\n",
    "def idf(word, token_doclist):\n",
    "    df = num_doc(word, token_doclist)\n",
    "    return math.log10(len(token_doclist)/df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_all = {\"doc1\":new_tokens1, \"doc2\":new_tokens1, \"doc3\": new_tokens3}\n",
    "no_doc_all = {\"doc1\":no_new_tokens1, \"doc2\":no_new_tokens1, \"doc3\": no_new_tokens3}\n",
    "\n",
    "def Tf_idf(doc_all):\n",
    "    # create bag words\n",
    "    bag_words = []\n",
    "    for doc in doc_all.keys():\n",
    "        bag_words += doc_all[doc]\n",
    "    bag_words = set(bag_words) # without repeat words\n",
    "    #calculate idf for every word in bag_words\n",
    "    bag_words_idf = {} # declare \"bag_words_idf\" data structure is dictionary \n",
    "    for word in bag_words:\n",
    "        bag_words_idf[word] = idf(word, doc_all.values())\n",
    "\n",
    "    tf_idf = {} # store tfidf value\n",
    "    for doc in doc_all.keys():\n",
    "        tf_idf_doc = {} # as a dictionary to store tfidf of each doc\n",
    "        for term in set(doc_all[doc]):\n",
    "            tf_idf_doc[term] = tf(term, doc_all[doc]) * bag_words_idf[term]\n",
    "        tf_idf[doc] = tf_idf_doc\n",
    "        \n",
    "    return tf_idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc1': {'John': 0.0020010370347236504, 'first': 0.0, 'created': 0.0020010370347236504, 'Strings': 0.0020010370347236504, 'makes': 0.0020010370347236504, 'get': 0.0, 'installed': 0.0020010370347236504, 'includes': 0.0020010370347236504, 'Nick': 0.0020010370347236504, 'written': 0.004002074069447301, 'enjoy': 0.0020010370347236504, 'Special': 0.0020010370347236504, 'sections': 0.004002074069447301, 'exercises': 0.0020010370347236504, 'materials': 0.004002074069447301, 'Steve': 0.0020010370347236504, 'internet': 0.0020010370347236504, 'started': 0.0, 'leading': 0.0020010370347236504, 'linked': 0.0020010370347236504, 'section': 0.004002074069447301, 'parallel': 0.0020010370347236504, 'coding': 0.0, 'Johnson': 0.0020010370347236504, 'starts': 0.0020010370347236504, 'introduction': 0.0020010370347236504, 'language': 0.0, 'link': 0.0020010370347236504, 'introducing': 0.0020010370347236504, 'intensive': 0.0020010370347236504, 'videos': 0.0020010370347236504, 'colleagues': 0.0020010370347236504, 'Maggie': 0.0020010370347236504, 'enlightened': 0.0020010370347236504, 'Commons': 0.0020010370347236504, 'thanks': 0.004002074069447301, 'Antoine': 0.0020010370347236504, 'Up': 0.0020010370347236504, 'Introduction': 0.0020010370347236504, 'lecture': 0.0020010370347236504, 'Cox': 0.0020010370347236504, 'group': 0.0020010370347236504, 'material': 0.008004148138894602, 'director': 0.0020010370347236504, 'strings': 0.0, 'Piotr': 0.0020010370347236504, 'Google': 0.008004148138894602, 'Creative': 0.0020010370347236504, 'Set': 0.0020010370347236504, 'Attribution': 0.0020010370347236504, 'class': 0.0020010370347236504, 'working': 0.0020010370347236504, 'Glassman': 0.0020010370347236504, 'engEDU': 0.0020010370347236504, 'help': 0.0020010370347236504, 'share': 0.0020010370347236504, 'Picard': 0.0020010370347236504, 'Parlante': 0.0020010370347236504, 'Kaminksi': 0.0020010370347236504, 'organized': 0.0020010370347236504, 'exercise': 0.004002074069447301, 'generosity': 0.0020010370347236504, 'machine': 0.0020010370347236504, 'put': 0.0020010370347236504, 'free': 0.0020010370347236504, 'code': 0.0020010370347236504, 'Python': 0.0, 'license': 0.0020010370347236504, 'left': 0.0020010370347236504}, 'doc2': {'John': 0.0020010370347236504, 'first': 0.0, 'created': 0.0020010370347236504, 'Strings': 0.0020010370347236504, 'makes': 0.0020010370347236504, 'get': 0.0, 'installed': 0.0020010370347236504, 'includes': 0.0020010370347236504, 'Nick': 0.0020010370347236504, 'written': 0.004002074069447301, 'enjoy': 0.0020010370347236504, 'Special': 0.0020010370347236504, 'sections': 0.004002074069447301, 'exercises': 0.0020010370347236504, 'materials': 0.004002074069447301, 'Steve': 0.0020010370347236504, 'internet': 0.0020010370347236504, 'started': 0.0, 'leading': 0.0020010370347236504, 'linked': 0.0020010370347236504, 'section': 0.004002074069447301, 'parallel': 0.0020010370347236504, 'coding': 0.0, 'Johnson': 0.0020010370347236504, 'starts': 0.0020010370347236504, 'introduction': 0.0020010370347236504, 'language': 0.0, 'link': 0.0020010370347236504, 'introducing': 0.0020010370347236504, 'intensive': 0.0020010370347236504, 'videos': 0.0020010370347236504, 'colleagues': 0.0020010370347236504, 'Maggie': 0.0020010370347236504, 'enlightened': 0.0020010370347236504, 'Commons': 0.0020010370347236504, 'thanks': 0.004002074069447301, 'Antoine': 0.0020010370347236504, 'Up': 0.0020010370347236504, 'Introduction': 0.0020010370347236504, 'lecture': 0.0020010370347236504, 'Cox': 0.0020010370347236504, 'group': 0.0020010370347236504, 'material': 0.008004148138894602, 'director': 0.0020010370347236504, 'strings': 0.0, 'Piotr': 0.0020010370347236504, 'Google': 0.008004148138894602, 'Creative': 0.0020010370347236504, 'Set': 0.0020010370347236504, 'Attribution': 0.0020010370347236504, 'class': 0.0020010370347236504, 'working': 0.0020010370347236504, 'Glassman': 0.0020010370347236504, 'engEDU': 0.0020010370347236504, 'help': 0.0020010370347236504, 'share': 0.0020010370347236504, 'Picard': 0.0020010370347236504, 'Parlante': 0.0020010370347236504, 'Kaminksi': 0.0020010370347236504, 'organized': 0.0020010370347236504, 'exercise': 0.004002074069447301, 'generosity': 0.0020010370347236504, 'machine': 0.0020010370347236504, 'put': 0.0020010370347236504, 'free': 0.0020010370347236504, 'code': 0.0020010370347236504, 'Python': 0.0, 'license': 0.0020010370347236504, 'left': 0.0020010370347236504}, 'doc3': {'development': 0.005022328997049078, 'first': 0.0, 'get': 0.0, 'Well': 0.005022328997049078, 'waiting': 0.005022328997049078, 'pro': 0.005022328997049078, 'easiest': 0.005022328997049078, 'course': 0.020089315988196313, 'major': 0.005022328997049078, 'whiz': 0.005022328997049078, 'started': 0.0, 'begin': 0.005022328997049078, 'cut': 0.005022328997049078, 'basics': 0.010044657994098156, 'coding': 0.0, 'file': 0.005022328997049078, 'small': 0.005022328997049078, 'place': 0.005022328997049078, 'great': 0.005022328997049078, 'incentives': 0.005022328997049078, 'language': 0.0, 'fret': 0.005022328997049078, 'Let': 0.010044657994098156, 'adapted': 0.005022328997049078, 'programming': 0.005022328997049078, 'types': 0.005022328997049078, 'loops': 0.005022328997049078, 'variables': 0.005022328997049078, 'fun': 0.005022328997049078, 'projects': 0.005022328997049078, 'coded': 0.005022328997049078, 'bit': 0.005022328997049078, 'right': 0.005022328997049078, 'concepts': 0.005022328997049078, 'getting': 0.005022328997049078, 'strings': 0.0, 'need': 0.015066986991147235, 'know': 0.015066986991147235, 'World': 0.005022328997049078, 'write': 0.005022328997049078, 'move': 0.005022328997049078, 'learn': 0.005022328997049078, 'conditions': 0.005022328997049078, 'Afterwards': 0.005022328997049078, 'faster': 0.005022328997049078, 'quick': 0.010044657994098156, 'excited': 0.005022328997049078, 'hope': 0.005022328997049078, 'friendly': 0.005022328997049078, 'discuss': 0.005022328997049078, 'learning': 0.015066986991147235, 'everything': 0.005022328997049078, 'use': 0.005022328997049078, 'time': 0.020089315988196313, 'large': 0.005022328997049078, 'dive': 0.005022328997049078, 'finish': 0.005022328997049078, 'data': 0.005022328997049078, 'languages': 0.010044657994098156, 'functions': 0.005022328997049078, 'manipulation': 0.005022328997049078, 'way': 0.005022328997049078, 'overall': 0.005022328997049078, 'Python': 0.0, 'much': 0.005022328997049078}}\n"
     ]
    }
   ],
   "source": [
    "print(Tf_idf(no_doc_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc1': {'first': 0.0, 'created': 0.0020475727797172236, 'makes': 0.0020475727797172236, 'get': 0.0, 'installed': 0.0020475727797172236, 'includes': 0.0020475727797172236, 'special': 0.0020475727797172236, 'engedu': 0.0020475727797172236, 'written': 0.004095145559434447, 'license': 0.0020475727797172236, 'enjoy': 0.0020475727797172236, 'sections': 0.004095145559434447, 'set': 0.0020475727797172236, 'exercises': 0.0020475727797172236, 'parlante': 0.0020475727797172236, 'materials': 0.004095145559434447, 'cox': 0.0020475727797172236, 'internet': 0.0020475727797172236, 'creative': 0.0020475727797172236, 'started': 0.0, 'python': 0.0, 'linked': 0.0020475727797172236, 'leading': 0.0020475727797172236, 'section': 0.004095145559434447, 'parallel': 0.0020475727797172236, 'coding': 0.0, 'nick': 0.0020475727797172236, 'starts': 0.0020475727797172236, 'introduction': 0.004095145559434447, 'language': 0.0, 'link': 0.0020475727797172236, 'introducing': 0.0020475727797172236, 'intensive': 0.0020475727797172236, 'videos': 0.0020475727797172236, 'colleagues': 0.0020475727797172236, 'piotr': 0.0020475727797172236, 'glassman': 0.0020475727797172236, 'maggie': 0.0020475727797172236, 'thanks': 0.004095145559434447, 'enlightened': 0.0020475727797172236, 'attribution': 0.0020475727797172236, 'lecture': 0.0020475727797172236, 'group': 0.0020475727797172236, 'material': 0.008190291118868894, 'director': 0.0020475727797172236, 'strings': 0.0, 'commons': 0.0020475727797172236, 'class': 0.0020475727797172236, 'working': 0.0020475727797172236, 'help': 0.0020475727797172236, 'share': 0.0020475727797172236, 'john': 0.0020475727797172236, 'organized': 0.0020475727797172236, 'steve': 0.0020475727797172236, 'exercise': 0.004095145559434447, 'johnson': 0.0020475727797172236, 'generosity': 0.0020475727797172236, 'machine': 0.0020475727797172236, 'put': 0.0020475727797172236, 'google': 0.008190291118868894, 'free': 0.0020475727797172236, 'code': 0.0020475727797172236, 'antoine': 0.0020475727797172236, 'picard': 0.0020475727797172236, 'left': 0.0020475727797172236}, 'doc2': {'first': 0.0, 'created': 0.0020475727797172236, 'makes': 0.0020475727797172236, 'get': 0.0, 'installed': 0.0020475727797172236, 'includes': 0.0020475727797172236, 'special': 0.0020475727797172236, 'engedu': 0.0020475727797172236, 'written': 0.004095145559434447, 'license': 0.0020475727797172236, 'enjoy': 0.0020475727797172236, 'sections': 0.004095145559434447, 'set': 0.0020475727797172236, 'exercises': 0.0020475727797172236, 'parlante': 0.0020475727797172236, 'materials': 0.004095145559434447, 'cox': 0.0020475727797172236, 'internet': 0.0020475727797172236, 'creative': 0.0020475727797172236, 'started': 0.0, 'python': 0.0, 'linked': 0.0020475727797172236, 'leading': 0.0020475727797172236, 'section': 0.004095145559434447, 'parallel': 0.0020475727797172236, 'coding': 0.0, 'nick': 0.0020475727797172236, 'starts': 0.0020475727797172236, 'introduction': 0.004095145559434447, 'language': 0.0, 'link': 0.0020475727797172236, 'introducing': 0.0020475727797172236, 'intensive': 0.0020475727797172236, 'videos': 0.0020475727797172236, 'colleagues': 0.0020475727797172236, 'piotr': 0.0020475727797172236, 'glassman': 0.0020475727797172236, 'maggie': 0.0020475727797172236, 'thanks': 0.004095145559434447, 'enlightened': 0.0020475727797172236, 'attribution': 0.0020475727797172236, 'lecture': 0.0020475727797172236, 'group': 0.0020475727797172236, 'material': 0.008190291118868894, 'director': 0.0020475727797172236, 'strings': 0.0, 'commons': 0.0020475727797172236, 'class': 0.0020475727797172236, 'working': 0.0020475727797172236, 'help': 0.0020475727797172236, 'share': 0.0020475727797172236, 'john': 0.0020475727797172236, 'organized': 0.0020475727797172236, 'steve': 0.0020475727797172236, 'exercise': 0.004095145559434447, 'johnson': 0.0020475727797172236, 'generosity': 0.0020475727797172236, 'machine': 0.0020475727797172236, 'put': 0.0020475727797172236, 'google': 0.008190291118868894, 'free': 0.0020475727797172236, 'code': 0.0020475727797172236, 'antoine': 0.0020475727797172236, 'picard': 0.0020475727797172236, 'left': 0.0020475727797172236}, 'doc3': {'development': 0.005243090711205082, 'first': 0.0, 'get': 0.0, 'waiting': 0.005243090711205082, 'pro': 0.005243090711205082, 'easiest': 0.005243090711205082, 'course': 0.02097236284482033, 'major': 0.005243090711205082, 'whiz': 0.005243090711205082, 'started': 0.0, 'python': 0.0, 'begin': 0.005243090711205082, 'cut': 0.005243090711205082, 'basics': 0.010486181422410165, 'coding': 0.0, 'file': 0.005243090711205082, 'small': 0.005243090711205082, 'place': 0.005243090711205082, 'great': 0.005243090711205082, 'incentives': 0.005243090711205082, 'language': 0.0, 'adapted': 0.005243090711205082, 'programming': 0.005243090711205082, 'types': 0.005243090711205082, 'loops': 0.005243090711205082, 'variables': 0.005243090711205082, 'fun': 0.005243090711205082, 'projects': 0.005243090711205082, 'coded': 0.005243090711205082, 'bit': 0.005243090711205082, 'right': 0.005243090711205082, 'concepts': 0.005243090711205082, 'getting': 0.005243090711205082, 'strings': 0.0, 'need': 0.015729272133615246, 'know': 0.015729272133615246, 'write': 0.005243090711205082, 'move': 0.005243090711205082, 'conditions': 0.005243090711205082, 'afterwards': 0.005243090711205082, 'faster': 0.005243090711205082, 'quick': 0.010486181422410165, 'excited': 0.005243090711205082, 'hope': 0.005243090711205082, 'friendly': 0.005243090711205082, 'discuss': 0.005243090711205082, 'learning': 0.015729272133615246, 'everything': 0.005243090711205082, 'use': 0.005243090711205082, 'time': 0.02097236284482033, 'large': 0.005243090711205082, 'let': 0.010486181422410165, 'dive': 0.005243090711205082, 'finish': 0.005243090711205082, 'data': 0.005243090711205082, 'languages': 0.010486181422410165, 'functions': 0.005243090711205082, 'world': 0.005243090711205082, 'manipulation': 0.005243090711205082, 'way': 0.005243090711205082, 'overall': 0.005243090711205082, 'much': 0.005243090711205082}}\n"
     ]
    }
   ],
   "source": [
    "print(Tf_idf(doc_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Display the result in the panda data frame as below (you have to display 2 results, one for non-normalization and one for normalization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non- normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Afterwards</th>\n",
       "      <th>Antoine</th>\n",
       "      <th>Attribution</th>\n",
       "      <th>Commons</th>\n",
       "      <th>Cox</th>\n",
       "      <th>Creative</th>\n",
       "      <th>Glassman</th>\n",
       "      <th>Google</th>\n",
       "      <th>Introduction</th>\n",
       "      <th>John</th>\n",
       "      <th>...</th>\n",
       "      <th>types</th>\n",
       "      <th>use</th>\n",
       "      <th>variables</th>\n",
       "      <th>videos</th>\n",
       "      <th>waiting</th>\n",
       "      <th>way</th>\n",
       "      <th>whiz</th>\n",
       "      <th>working</th>\n",
       "      <th>write</th>\n",
       "      <th>written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.008004</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.008004</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc3</th>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Afterwards   Antoine  Attribution   Commons       Cox  Creative  \\\n",
       "doc1         NaN  0.002001     0.002001  0.002001  0.002001  0.002001   \n",
       "doc2         NaN  0.002001     0.002001  0.002001  0.002001  0.002001   \n",
       "doc3    0.005022       NaN          NaN       NaN       NaN       NaN   \n",
       "\n",
       "      Glassman    Google  Introduction      John  ...     types       use  \\\n",
       "doc1  0.002001  0.008004      0.002001  0.002001  ...       NaN       NaN   \n",
       "doc2  0.002001  0.008004      0.002001  0.002001  ...       NaN       NaN   \n",
       "doc3       NaN       NaN           NaN       NaN  ...  0.005022  0.005022   \n",
       "\n",
       "      variables    videos   waiting       way      whiz   working     write  \\\n",
       "doc1        NaN  0.002001       NaN       NaN       NaN  0.002001       NaN   \n",
       "doc2        NaN  0.002001       NaN       NaN       NaN  0.002001       NaN   \n",
       "doc3   0.005022       NaN  0.005022  0.005022  0.005022       NaN  0.005022   \n",
       "\n",
       "       written  \n",
       "doc1  0.004002  \n",
       "doc2  0.004002  \n",
       "doc3       NaN  \n",
       "\n",
       "[3 rows x 127 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "no_tf_idf = Tf_idf(no_doc_all)\n",
    "no_tf_dataframe = pd.DataFrame(no_tf_idf).transpose()\n",
    "no_tf_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adapted</th>\n",
       "      <th>afterwards</th>\n",
       "      <th>antoine</th>\n",
       "      <th>attribution</th>\n",
       "      <th>basics</th>\n",
       "      <th>begin</th>\n",
       "      <th>bit</th>\n",
       "      <th>class</th>\n",
       "      <th>code</th>\n",
       "      <th>coded</th>\n",
       "      <th>...</th>\n",
       "      <th>use</th>\n",
       "      <th>variables</th>\n",
       "      <th>videos</th>\n",
       "      <th>waiting</th>\n",
       "      <th>way</th>\n",
       "      <th>whiz</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>write</th>\n",
       "      <th>written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc3</th>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010486</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       adapted  afterwards   antoine  attribution    basics     begin  \\\n",
       "doc1       NaN         NaN  0.002048     0.002048       NaN       NaN   \n",
       "doc2       NaN         NaN  0.002048     0.002048       NaN       NaN   \n",
       "doc3  0.005243    0.005243       NaN          NaN  0.010486  0.005243   \n",
       "\n",
       "           bit     class      code     coded  ...       use  variables  \\\n",
       "doc1       NaN  0.002048  0.002048       NaN  ...       NaN        NaN   \n",
       "doc2       NaN  0.002048  0.002048       NaN  ...       NaN        NaN   \n",
       "doc3  0.005243       NaN       NaN  0.005243  ...  0.005243   0.005243   \n",
       "\n",
       "        videos   waiting       way      whiz   working     world     write  \\\n",
       "doc1  0.002048       NaN       NaN       NaN  0.002048       NaN       NaN   \n",
       "doc2  0.002048       NaN       NaN       NaN  0.002048       NaN       NaN   \n",
       "doc3       NaN  0.005243  0.005243  0.005243       NaN  0.005243  0.005243   \n",
       "\n",
       "       written  \n",
       "doc1  0.004095  \n",
       "doc2  0.004095  \n",
       "doc3       NaN  \n",
       "\n",
       "[3 rows x 120 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = Tf_idf(doc_all)\n",
    "tf_dataframe = pd.DataFrame(tf_idf).transpose()\n",
    "tf_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Assume that the goal is to distinguish the differences between the three documents based on their features (term). Doing a feature selection based on POS in question 2 compared to not implementing a feature selection, do you think which option would be better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that doing a feature selection based on POS is better than without feature selection.\n",
    "Since if we can delete those non-important terms and get more focus on those meaningful terms,\n",
    "it indeed saves our time.\n",
    "By pick those useful and unique terms out, we can clearly see where are the differencies and how they distributed.\n",
    "Also, use POS method, we can choose which terms we want to focus on this time and modify the filter in different situation.\n",
    "Therefore, I think we should do feature selection with POS method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Feature Selection?\n",
    "\n",
    "1. Curse of dimensionality — Overfitting:\n",
    "If we have more columns in the data than the number of rows, we will be able to fit our training data perfectly, but that won’t generalize to the new samples. And thus we learn absolutely nothing.\n",
    "2. Occam’s Razor:\n",
    "We want our models to be simple and explainable. We lose explainability when we have a lot of features.\n",
    "3. Garbage In Garbage out:\n",
    "Most of the times, we will have many non-informative features. For Example, Name or ID variables. Poor-quality input will produce Poor-Quality output.\n",
    "Also, a large number of features make a model bulky, time-taking, and harder to implement in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Based on the content of the 3 document files above, do you suggest a better feature selection method? (use POS but choose other POS tags or a different method or combination of methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think using POS by choosing noun, verb and adjective are enough since they are the most frequently used terms.\n",
    "Also, since the words in 3 documents are quite similar, so there's no meaning to choose other more tags.\n",
    "If we want to imporve the performance, I think we should use other methods. \n",
    "Below is the method I recommend and I think that the result will be much better than only doing POS feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two-stage feature selection method\n",
    "The proposed method is a combination of FCD and LSI. \n",
    "Firstly, we select features by the FCD feature selection method to reduce the feature numbers observably. \n",
    "Secondly, we apply LSI to construct a new conceptual vector space. \n",
    "The two-stage feature selection method conjugates the vector space model and the semantic feature space model. \n",
    "The related strategy for the proposed method is stated in detail as follows:\n",
    "\n",
    "Step 1. Remove stop words, punctuation, and non-alphanumeric text.\n",
    "\n",
    "Step 2. Calculate the normalized TFIDF in the corresponding element of the weight matrix.\n",
    "\n",
    "Step 3. Select the features according to the FCD method and get a new vector space model.\n",
    "\n",
    "Step 4. Construct the new semantic space model by means of LSI.\n",
    "\n",
    "Step 5. Use the SVM classifier on the semantic space model.\n",
    "\n",
    "Step 6. Obtain the categorization performance over the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
