{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4- POS and vectorize plain text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization & POS tagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Read 3 document (doc1.txt, doc2.txt, doc3.txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "file1 = read_data('./data/doc1.txt')\n",
    "file2 = read_data('./data/doc2.txt')\n",
    "file3 = read_data('./data/doc3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Normalize words: convert all upper-case letter to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to get started, the python sections are linked at the left -- python set up to get python installed on your machine, python introduction for an introduction to the language, and then python strings starts the coding material, leading to the first exercise. the end of each written section includes a link to the code exercise for that section's material. the lecture videos parallel the written materials, introducing python, then strings, then first exercises, and so on. at google, all this material makes up an intensive 2-day class, so the videos are organized as the day-1 and day-2 sections.\n",
      "\n",
      "this material was created by nick parlante working in the engedu group at google. special thanks for the help from my google colleagues john cox, steve glassman, piotr kaminksi, and antoine picard. and finally thanks to google and my director maggie johnson for the enlightened generosity to put these materials out on the internet for free under the creative commons attribution 2.5 license -- share and enjoy!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def normalized(file):\n",
    "    file = file.lower()\n",
    "    return file\n",
    "\n",
    "norm_file1 = normalized(file1)\n",
    "norm_file2 = normalized(file2)\n",
    "norm_file3 = normalized(file3)\n",
    "print(norm_file1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c)Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'get', 'started', ',', 'the', 'python', 'sections', 'are', 'linked', 'at', 'the', 'left', '--', 'python', 'set', 'up', 'to', 'get', 'python', 'installed', 'on', 'your', 'machine', ',', 'python', 'introduction', 'for', 'an', 'introduction', 'to', 'the', 'language', ',', 'and', 'then', 'python', 'strings', 'starts', 'the', 'coding', 'material', ',', 'leading', 'to', 'the', 'first', 'exercise', '.', 'the', 'end', 'of', 'each', 'written', 'section', 'includes', 'a', 'link', 'to', 'the', 'code', 'exercise', 'for', 'that', 'section', \"'s\", 'material', '.', 'the', 'lecture', 'videos', 'parallel', 'the', 'written', 'materials', ',', 'introducing', 'python', ',', 'then', 'strings', ',', 'then', 'first', 'exercises', ',', 'and', 'so', 'on', '.', 'at', 'google', ',', 'all', 'this', 'material', 'makes', 'up', 'an', 'intensive', '2-day', 'class', ',', 'so', 'the', 'videos', 'are', 'organized', 'as', 'the', 'day-1', 'and', 'day-2', 'sections', '.', 'this', 'material', 'was', 'created', 'by', 'nick', 'parlante', 'working', 'in', 'the', 'engedu', 'group', 'at', 'google', '.', 'special', 'thanks', 'for', 'the', 'help', 'from', 'my', 'google', 'colleagues', 'john', 'cox', ',', 'steve', 'glassman', ',', 'piotr', 'kaminksi', ',', 'and', 'antoine', 'picard', '.', 'and', 'finally', 'thanks', 'to', 'google', 'and', 'my', 'director', 'maggie', 'johnson', 'for', 'the', 'enlightened', 'generosity', 'to', 'put', 'these', 'materials', 'out', 'on', 'the', 'internet', 'for', 'free', 'under', 'the', 'creative', 'commons', 'attribution', '2.5', 'license', '--', 'share', 'and', 'enjoy', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "token1 = nltk.word_tokenize(norm_file1)\n",
    "token2 = nltk.word_tokenize(norm_file2)\n",
    "token3 = nltk.word_tokenize(norm_file3)\n",
    "# non normalized\n",
    "no_token1 = nltk.word_tokenize(file1)\n",
    "no_token2 = nltk.word_tokenize(file2)\n",
    "no_token3 = nltk.word_tokenize(file3)\n",
    "print(token1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Remove stopwords using stopwords from nltk or other sources and remove punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'started', ',', 'python', 'sections', 'linked', 'left', '--', 'python', 'set', 'get', 'python', 'installed', 'machine', ',', 'python', 'introduction', 'introduction', 'language', ',', 'python', 'strings', 'starts', 'coding', 'material', ',', 'leading', 'first', 'exercise', '.', 'written', 'section', 'includes', 'link', 'code', 'exercise', 'section', \"'s\", 'material', '.', 'lecture', 'videos', 'parallel', 'written', 'materials', ',', 'introducing', 'python', ',', 'strings', ',', 'first', 'exercises', ',', '.', 'google', ',', 'material', 'makes', 'intensive', '2-day', 'class', ',', 'videos', 'organized', 'day-1', 'day-2', 'sections', '.', 'material', 'created', 'nick', 'parlante', 'working', 'engedu', 'group', 'google', '.', 'special', 'thanks', 'help', 'google', 'colleagues', 'john', 'cox', ',', 'steve', 'glassman', ',', 'piotr', 'kaminksi', ',', 'antoine', 'picard', '.', 'finally', 'thanks', 'google', 'director', 'maggie', 'johnson', 'enlightened', 'generosity', 'put', 'materials', 'internet', 'free', 'creative', 'commons', 'attribution', '2.5', 'license', '--', 'share', 'enjoy', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword = nltk.download('stopwords')\n",
    "\n",
    "tokens1_no_sw = [word for word in token1 if not word in stopwords.words()]\n",
    "tokens2_no_sw = [word for word in token2 if not word in stopwords.words()]\n",
    "tokens3_no_sw = [word for word in token3 if not word in stopwords.words()]\n",
    "# non normalized\n",
    "no_tokens1_no_sw = [word for word in no_token1 if not word in stopwords.words()]\n",
    "no_tokens2_no_sw = [word for word in no_token2 if not word in stopwords.words()]\n",
    "no_tokens3_no_sw = [word for word in no_token3 if not word in stopwords.words()]\n",
    "print(tokens1_no_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['get', 'started', 'python', 'sections', 'linked', 'left', 'python', 'set', 'get', 'python', 'installed', 'machine', 'python', 'introduction', 'introduction', 'language', 'python', 'strings', 'starts', 'coding', 'material', 'leading', 'first', 'exercise', 'written', 'section', 'includes', 'link', 'code', 'exercise', 'section', 'material', 'lecture', 'videos', 'parallel', 'written', 'materials', 'introducing', 'python', 'strings', 'first', 'exercises', 'google', 'material', 'makes', 'intensive', 'class', 'videos', 'organized', 'sections', 'material', 'created', 'nick', 'parlante', 'working', 'engedu', 'group', 'google', 'special', 'thanks', 'help', 'google', 'colleagues', 'john', 'cox', 'steve', 'glassman', 'piotr', 'kaminksi', 'antoine', 'picard', 'finally', 'thanks', 'google', 'director', 'maggie', 'johnson', 'enlightened', 'generosity', 'put', 'materials', 'internet', 'free', 'creative', 'commons', 'attribution', 'license', 'share', 'enjoy']\n"
     ]
    }
   ],
   "source": [
    "words1 = [word for word in tokens1_no_sw if word.isalpha()]\n",
    "words2 = [word for word in tokens2_no_sw if word.isalpha()]\n",
    "words3 = [word for word in tokens3_no_sw if word.isalpha()]\n",
    "# non normalized\n",
    "no_words1 = [word for word in no_tokens1_no_sw if word.isalpha()]\n",
    "no_words2 = [word for word in no_tokens2_no_sw if word.isalpha()]\n",
    "no_words3 = [word for word in no_tokens3_no_sw if word.isalpha()]\n",
    "print(words1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) POS tagging for every term and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('universal_tagset')\n",
    "pos1_tokens_uni = nltk.pos_tag(words1,tagset='universal')\n",
    "pos2_tokens_uni = nltk.pos_tag(words2,tagset='universal')\n",
    "pos3_tokens_uni = nltk.pos_tag(words3,tagset='universal')\n",
    "# non normalized\n",
    "no_pos1_tokens_uni = nltk.pos_tag(no_words1,tagset='universal')\n",
    "no_pos2_tokens_uni = nltk.pos_tag(no_words2,tagset='universal')\n",
    "no_pos3_tokens_uni = nltk.pos_tag(no_words3,tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('get', 'NOUN'), ('started', 'VERB'), ('python', 'ADJ'), ('sections', 'NOUN'), ('linked', 'VERB'), ('left', 'ADJ'), ('python', 'NOUN'), ('set', 'VERB'), ('get', 'VERB'), ('python', 'ADJ'), ('installed', 'VERB'), ('machine', 'NOUN'), ('python', 'NOUN'), ('introduction', 'NOUN'), ('introduction', 'NOUN'), ('language', 'NOUN'), ('python', 'NOUN'), ('strings', 'NOUN'), ('starts', 'VERB'), ('coding', 'VERB'), ('material', 'NOUN'), ('leading', 'VERB'), ('first', 'ADJ'), ('exercise', 'NOUN'), ('written', 'VERB'), ('section', 'NOUN'), ('includes', 'VERB'), ('link', 'VERB'), ('code', 'NOUN'), ('exercise', 'NOUN'), ('section', 'NOUN'), ('material', 'NOUN'), ('lecture', 'NOUN'), ('videos', 'X'), ('parallel', 'ADJ'), ('written', 'VERB'), ('materials', 'NOUN'), ('introducing', 'VERB'), ('python', 'NOUN'), ('strings', 'NOUN'), ('first', 'ADJ'), ('exercises', 'VERB'), ('google', 'ADJ'), ('material', 'NOUN'), ('makes', 'VERB'), ('intensive', 'ADJ'), ('class', 'NOUN'), ('videos', 'NOUN'), ('organized', 'VERB'), ('sections', 'NOUN'), ('material', 'NOUN'), ('created', 'VERB'), ('nick', 'ADJ'), ('parlante', 'NOUN'), ('working', 'VERB'), ('engedu', 'ADJ'), ('group', 'NOUN'), ('google', 'VERB'), ('special', 'ADJ'), ('thanks', 'NOUN'), ('help', 'VERB'), ('google', 'VERB'), ('colleagues', 'NOUN'), ('john', 'VERB'), ('cox', 'NOUN'), ('steve', 'VERB'), ('glassman', 'NOUN'), ('piotr', 'NOUN'), ('kaminksi', 'X'), ('antoine', 'ADJ'), ('picard', 'NOUN'), ('finally', 'ADV'), ('thanks', 'NOUN'), ('google', 'VERB'), ('director', 'NOUN'), ('maggie', 'NOUN'), ('johnson', 'NOUN'), ('enlightened', 'VERB'), ('generosity', 'NOUN'), ('put', 'VERB'), ('materials', 'NOUN'), ('internet', 'VERB'), ('free', 'ADJ'), ('creative', 'ADJ'), ('commons', 'NOUN'), ('attribution', 'NOUN'), ('license', 'NOUN'), ('share', 'NOUN'), ('enjoy', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "print(pos1_tokens_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('welcome', 'ADJ'), ('google', 'NOUN'), ('python', 'NOUN'), ('online', 'VERB'), ('tutorial', 'NOUN'), ('based', 'VERB'), ('introductory', 'ADJ'), ('python', 'ADJ'), ('course', 'NOUN'), ('offered', 'VERB'), ('internally', 'ADV'), ('originally', 'ADV'), ('created', 'VERB'), ('python', 'NOUN'), ('days', 'NOUN'), ('tried', 'VERB'), ('keep', 'ADJ'), ('content', 'NOUN'), ('universal', 'NOUN'), ('exercises', 'VERB'), ('relevant', 'ADJ'), ('even', 'ADV'), ('newer', 'ADJ'), ('releases', 'NOUN'), ('mentioned', 'VERB'), ('setup', 'ADJ'), ('page', 'NOUN'), ('material', 'NOUN'), ('covers', 'VERB'), ('python', 'ADJ'), ('recommend', 'NOUN'), ('avoiding', 'VERB'), ('python', 'ADJ'), ('recognize', 'VERB'), ('future', 'ADJ'), ('new', 'ADJ'), ('features', 'NOUN'), ('going', 'VERB'), ('good', 'ADJ'), ('news', 'NOUN'), ('developers', 'NOUN'), ('learning', 'VERB'), ('either', 'DET'), ('version', 'NOUN'), ('pick', 'NOUN'), ('without', 'ADP'), ('much', 'ADJ'), ('difficulty', 'NOUN'), ('know', 'VERB'), ('choosing', 'VERB'), ('python', 'NOUN'), ('check', 'NOUN'), ('post', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "print(pos2_tokens_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('need', 'NOUN'), ('quick', 'ADJ'), ('learning', 'VERB'), ('python', 'NOUN'), ('first', 'ADJ'), ('time', 'NOUN'), ('right', 'ADJ'), ('place', 'NOUN'), ('let', 'VERB'), ('get', 'VERB'), ('started', 'VERB'), ('learning', 'VERB'), ('easiest', 'ADJ'), ('coding', 'VERB'), ('languages', 'NOUN'), ('right', 'ADV'), ('need', 'VERB'), ('fret', 'ADV'), ('coded', 'ADJ'), ('time', 'NOUN'), ('finish', 'ADJ'), ('course', 'NOUN'), ('pro', 'ADJ'), ('python', 'NOUN'), ('python', 'NOUN'), ('great', 'ADJ'), ('friendly', 'ADJ'), ('language', 'NOUN'), ('use', 'NOUN'), ('learn', 'X'), ('fun', 'NOUN'), ('adapted', 'VERB'), ('small', 'ADJ'), ('large', 'ADJ'), ('projects', 'NOUN'), ('python', 'VERB'), ('cut', 'VERB'), ('development', 'NOUN'), ('time', 'NOUN'), ('greatly', 'ADV'), ('overall', 'ADJ'), ('much', 'ADJ'), ('faster', 'ADJ'), ('write', 'ADJ'), ('python', 'NOUN'), ('languages', 'NOUN'), ('course', 'NOUN'), ('quick', 'ADJ'), ('way', 'NOUN'), ('understand', 'ADP'), ('major', 'ADJ'), ('concepts', 'NOUN'), ('python', 'VERB'), ('programming', 'VERB'), ('whiz', 'ADJ'), ('time', 'NOUN'), ('course', 'NOUN'), ('everything', 'NOUN'), ('need', 'NOUN'), ('know', 'VERB'), ('get', 'VERB'), ('started', 'VERB'), ('python', 'ADV'), ('along', 'ADP'), ('incentives', 'NOUN'), ('begin', 'VERB'), ('basics', 'NOUN'), ('python', 'VERB'), ('learning', 'VERB'), ('strings', 'NOUN'), ('variables', 'NOUN'), ('getting', 'VERB'), ('know', 'VERB'), ('data', 'NOUN'), ('types', 'NOUN'), ('soon', 'ADV'), ('move', 'VERB'), ('loops', 'ADJ'), ('conditions', 'NOUN'), ('python', 'VERB'), ('afterwards', 'NOUN'), ('discuss', 'ADJ'), ('bit', 'NOUN'), ('file', 'ADJ'), ('manipulation', 'NOUN'), ('functions', 'NOUN'), ('know', 'VERB'), ('basics', 'NOUN'), ('python', 'VERB'), ('hope', 'NOUN'), ('excited', 'VERB'), ('dive', 'ADJ'), ('world', 'NOUN'), ('python', 'NOUN'), ('course', 'NOUN'), ('well', 'ADV'), ('waiting', 'VERB'), ('let', 'VERB'), ('get', 'VERB'), ('started', 'VERB')]\n"
     ]
    }
   ],
   "source": [
    "print(pos3_tokens_uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature selection and tf-idf weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Using the result from question 1, do feature selection by only select the term with POS is Noun, Verb, and Adjective. Note that depending on which POS tagging method you use, you need to convert between POS tag and POS, (e.g. VB, VBZ, VBD are all VERBs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_terms(tokens):\n",
    "    NVA_tokens =[] \n",
    "    for token in tokens:\n",
    "        if token[1] == \"NOUN\":\n",
    "            NVA_tokens += [token[0]]\n",
    "        elif token[1] == \"VERB\":\n",
    "            NVA_tokens += [token[0]]\n",
    "        elif token[1] == \"ADJ\":\n",
    "            NVA_tokens += [token[0]]\n",
    "    return NVA_tokens\n",
    "\n",
    "new_tokens1 = extract_terms(pos1_tokens_uni)\n",
    "new_tokens2 = extract_terms(pos2_tokens_uni)\n",
    "new_tokens3 = extract_terms(pos3_tokens_uni)\n",
    "\n",
    "no_new_tokens1 = extract_terms(no_pos1_tokens_uni)\n",
    "no_new_tokens2 = extract_terms(no_pos2_tokens_uni)\n",
    "no_new_tokens3 = extract_terms(no_pos3_tokens_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before extract: 86\n",
      "After extract: 89\n",
      "['get', 'started', 'python', 'sections', 'linked', 'left', 'python', 'set', 'get', 'python', 'installed', 'machine', 'python', 'introduction', 'introduction', 'language', 'python', 'strings', 'starts', 'coding', 'material', 'leading', 'first', 'exercise', 'written', 'section', 'includes', 'link', 'code', 'exercise', 'section', 'material', 'lecture', 'parallel', 'written', 'materials', 'introducing', 'python', 'strings', 'first', 'exercises', 'google', 'material', 'makes', 'intensive', 'class', 'videos', 'organized', 'sections', 'material', 'created', 'nick', 'parlante', 'working', 'engedu', 'group', 'google', 'special', 'thanks', 'help', 'google', 'colleagues', 'john', 'cox', 'steve', 'glassman', 'piotr', 'antoine', 'picard', 'thanks', 'google', 'director', 'maggie', 'johnson', 'enlightened', 'generosity', 'put', 'materials', 'internet', 'free', 'creative', 'commons', 'attribution', 'license', 'share', 'enjoy']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before extract:\",len(new_tokens1))\n",
    "print(\"After extract:\",len(pos1_tokens_uni))\n",
    "print(new_tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before extract: 48\n",
      "After extract: 53\n",
      "['welcome', 'google', 'python', 'online', 'tutorial', 'based', 'introductory', 'python', 'course', 'offered', 'created', 'python', 'days', 'tried', 'keep', 'content', 'universal', 'exercises', 'relevant', 'newer', 'releases', 'mentioned', 'setup', 'page', 'material', 'covers', 'python', 'recommend', 'avoiding', 'python', 'recognize', 'future', 'new', 'features', 'going', 'good', 'news', 'developers', 'learning', 'version', 'pick', 'much', 'difficulty', 'know', 'choosing', 'python', 'check', 'post']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before extract:\",len(new_tokens2))\n",
    "print(\"After extract:\",len(pos2_tokens_uni))\n",
    "print(new_tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before extract: 91\n",
      "After extract: 100\n",
      "['need', 'quick', 'learning', 'python', 'first', 'time', 'right', 'place', 'let', 'get', 'started', 'learning', 'easiest', 'coding', 'languages', 'need', 'coded', 'time', 'finish', 'course', 'pro', 'python', 'python', 'great', 'friendly', 'language', 'use', 'fun', 'adapted', 'small', 'large', 'projects', 'python', 'cut', 'development', 'time', 'overall', 'much', 'faster', 'write', 'python', 'languages', 'course', 'quick', 'way', 'major', 'concepts', 'python', 'programming', 'whiz', 'time', 'course', 'everything', 'need', 'know', 'get', 'started', 'incentives', 'begin', 'basics', 'python', 'learning', 'strings', 'variables', 'getting', 'know', 'data', 'types', 'move', 'loops', 'conditions', 'python', 'afterwards', 'discuss', 'bit', 'file', 'manipulation', 'functions', 'know', 'basics', 'python', 'hope', 'excited', 'dive', 'world', 'python', 'course', 'waiting', 'let', 'get', 'started']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before extract:\",len(new_tokens3))\n",
    "print(\"After extract:\",len(pos3_tokens_uni))\n",
    "print(new_tokens3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) After removing all terms that are not Verb, Noun, Adjective, compute tf-idf weighting for every term in the three documents for both cases: nonnormalization (tf * idf) and normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf function\n",
    "def tf(term, token_doc):\n",
    "    tf = token_doc.count(term) / len(token_doc)\n",
    "    return tf\n",
    "\n",
    "# create function to calculate how many doc contain the term \n",
    "def num_doc(word, token_doclist):\n",
    "    doc_count = 0\n",
    "    for doc_token in token_doclist:\n",
    "        if doc_token.count(word) > 0:\n",
    "            doc_count += 1\n",
    "    return doc_count\n",
    "\n",
    "import math \n",
    "# create function to calculate Inverse Document Frequency in doclist\n",
    "def idf(word, token_doclist):\n",
    "    df = num_doc(word, token_doclist)\n",
    "    return math.log10(len(token_doclist)/df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_all = {\"doc1\":new_tokens1, \"doc2\":new_tokens1, \"doc3\": new_tokens3}\n",
    "no_doc_all = {\"doc1\":no_new_tokens1, \"doc2\":no_new_tokens1, \"doc3\": no_new_tokens3}\n",
    "\n",
    "def Tf_idf(doc_all):\n",
    "    # create bag words\n",
    "    bag_words = []\n",
    "    for doc in doc_all.keys():\n",
    "        bag_words += doc_all[doc]\n",
    "    bag_words = set(bag_words) # without repeat words\n",
    "    #calculate idf for every word in bag_words\n",
    "    bag_words_idf = {} # declare \"bag_words_idf\" data structure is dictionary \n",
    "    for word in bag_words:\n",
    "        bag_words_idf[word] = idf(word, doc_all.values())\n",
    "\n",
    "    tf_idf = {} # store tfidf value\n",
    "    for doc in doc_all.keys():\n",
    "        tf_idf_doc = {} # as a dictionary to store tfidf of each doc\n",
    "        for term in set(doc_all[doc]):\n",
    "            tf_idf_doc[term] = tf(term, doc_all[doc]) * bag_words_idf[term]\n",
    "        tf_idf[doc] = tf_idf_doc\n",
    "        \n",
    "    return tf_idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Display the result in the panda data frame as below (you have to display 2 results, one for non-normalization and one for normalization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non- normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Afterwards</th>\n",
       "      <th>Antoine</th>\n",
       "      <th>Attribution</th>\n",
       "      <th>Commons</th>\n",
       "      <th>Cox</th>\n",
       "      <th>Creative</th>\n",
       "      <th>Glassman</th>\n",
       "      <th>Google</th>\n",
       "      <th>Introduction</th>\n",
       "      <th>John</th>\n",
       "      <th>...</th>\n",
       "      <th>types</th>\n",
       "      <th>use</th>\n",
       "      <th>variables</th>\n",
       "      <th>videos</th>\n",
       "      <th>waiting</th>\n",
       "      <th>way</th>\n",
       "      <th>whiz</th>\n",
       "      <th>working</th>\n",
       "      <th>write</th>\n",
       "      <th>written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.008004</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.008004</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc3</th>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Afterwards   Antoine  Attribution   Commons       Cox  Creative  \\\n",
       "doc1         NaN  0.002001     0.002001  0.002001  0.002001  0.002001   \n",
       "doc2         NaN  0.002001     0.002001  0.002001  0.002001  0.002001   \n",
       "doc3    0.005022       NaN          NaN       NaN       NaN       NaN   \n",
       "\n",
       "      Glassman    Google  Introduction      John  ...     types       use  \\\n",
       "doc1  0.002001  0.008004      0.002001  0.002001  ...       NaN       NaN   \n",
       "doc2  0.002001  0.008004      0.002001  0.002001  ...       NaN       NaN   \n",
       "doc3       NaN       NaN           NaN       NaN  ...  0.005022  0.005022   \n",
       "\n",
       "      variables    videos   waiting       way      whiz   working     write  \\\n",
       "doc1        NaN  0.002001       NaN       NaN       NaN  0.002001       NaN   \n",
       "doc2        NaN  0.002001       NaN       NaN       NaN  0.002001       NaN   \n",
       "doc3   0.005022       NaN  0.005022  0.005022  0.005022       NaN  0.005022   \n",
       "\n",
       "       written  \n",
       "doc1  0.004002  \n",
       "doc2  0.004002  \n",
       "doc3       NaN  \n",
       "\n",
       "[3 rows x 127 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "no_tf_idf = Tf_idf(no_doc_all)\n",
    "no_tf_dataframe = pd.DataFrame(no_tf_idf).transpose()\n",
    "no_tf_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adapted</th>\n",
       "      <th>afterwards</th>\n",
       "      <th>antoine</th>\n",
       "      <th>attribution</th>\n",
       "      <th>basics</th>\n",
       "      <th>begin</th>\n",
       "      <th>bit</th>\n",
       "      <th>class</th>\n",
       "      <th>code</th>\n",
       "      <th>coded</th>\n",
       "      <th>...</th>\n",
       "      <th>use</th>\n",
       "      <th>variables</th>\n",
       "      <th>videos</th>\n",
       "      <th>waiting</th>\n",
       "      <th>way</th>\n",
       "      <th>whiz</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>write</th>\n",
       "      <th>written</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002048</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc3</th>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.010486</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>0.005243</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       adapted  afterwards   antoine  attribution    basics     begin  \\\n",
       "doc1       NaN         NaN  0.002048     0.002048       NaN       NaN   \n",
       "doc2       NaN         NaN  0.002048     0.002048       NaN       NaN   \n",
       "doc3  0.005243    0.005243       NaN          NaN  0.010486  0.005243   \n",
       "\n",
       "           bit     class      code     coded  ...       use  variables  \\\n",
       "doc1       NaN  0.002048  0.002048       NaN  ...       NaN        NaN   \n",
       "doc2       NaN  0.002048  0.002048       NaN  ...       NaN        NaN   \n",
       "doc3  0.005243       NaN       NaN  0.005243  ...  0.005243   0.005243   \n",
       "\n",
       "        videos   waiting       way      whiz   working     world     write  \\\n",
       "doc1  0.002048       NaN       NaN       NaN  0.002048       NaN       NaN   \n",
       "doc2  0.002048       NaN       NaN       NaN  0.002048       NaN       NaN   \n",
       "doc3       NaN  0.005243  0.005243  0.005243       NaN  0.005243  0.005243   \n",
       "\n",
       "       written  \n",
       "doc1  0.004095  \n",
       "doc2  0.004095  \n",
       "doc3       NaN  \n",
       "\n",
       "[3 rows x 120 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = Tf_idf(doc_all)\n",
    "tf_dataframe = pd.DataFrame(tf_idf).transpose()\n",
    "tf_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Assume that the goal is to distinguish the differences between the three documents based on their features (term). Doing a feature selection based on POS in question 2 compared to not implementing a feature selection, do you think which option would be better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Based on the content of the 3 document files above, do you suggest a better feature selection method? (use POS but choose other POS tags or a different method or combination of methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
